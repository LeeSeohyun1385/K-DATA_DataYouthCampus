{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선형 회귀식의 계수를 찾는 법 - OLS VS. SGD\n",
    "- 보스턴 집값 데이터 활용(RM VS Price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필요한 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 필요한 라이브러리 import \n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LinearRegression 모델을 사용한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##최소제곱법 방법으로 다중회귀계수 찾기 ( 전에 한 내용 ) -OLS방법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.46109164] -30.571032410898315\n",
      "y = 8.461092X + (-30.571)\n",
      "MSE: 36.517\n",
      "RMSE:  6.043\n",
      "R2:  0.602\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# 데이터 수집\n",
    "boston = load_boston()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df =pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "\n",
    "X = pd.DataFrame(df['RM'])\n",
    "y = boston.target\n",
    "\n",
    "# 전체 데이터 중 80%는 학습용, 20%는 검증용으로 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\\\n",
    "                                                   random_state=1)\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#모델 객체 생성\n",
    "reg = LinearRegression()\n",
    "\n",
    "# 모델 학습\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# 계수 및 절편 확인: _속성은 학습을 통해 결정되는 속성\n",
    "print(reg.coef_, reg.intercept_)\n",
    "\n",
    "#회귀식\n",
    "print(\"y = {:2f}X + ({:.3f})\".format(reg.coef_[0], reg.intercept_))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 예측 수행\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# MSE, RMSE, r2_score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"MSE:\", np.round(mse, 3))\n",
    "print(\"RMSE: \", np.round(rmse, 3))\n",
    "print(\"R2: \", np.round(r2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.92655718] [-2.73013646]\n",
      "y = 3.926557X + (-2.730)\n",
      "MSE: 57.374\n",
      "RMSE:  7.575\n",
      "R2:  0.374\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# 데이터 수집\n",
    "boston = load_boston()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df =pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "\n",
    "X = pd.DataFrame(df['RM'])\n",
    "y = boston.target\n",
    "\n",
    "# 전체 데이터 중 80%는 학습용, 20%는 검증용으로 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\\\n",
    "                                                   random_state=1)\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor  #이부분은 OLS와 다름 \n",
    "\n",
    "#모델 객체 생성\n",
    "reg = SGDRegressor()  #이부분은 OLS와 다름 \n",
    "\n",
    "# 모델 학습\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# 계수 및 절편 확인: _속성은 학습을 통해 결정되는 속성\n",
    "print(reg.coef_, reg.intercept_)\n",
    "\n",
    "#회귀식\n",
    "print(\"y = {:2f}X + ({:.3f})\".format(reg.coef_[0], reg.intercept_[0])) #이부분은 OLS와 다름 \n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 예측 수행\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# MSE, RMSE, r2_score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"MSE:\", np.round(mse, 3))\n",
    "print(\"RMSE: \", np.round(rmse, 3))\n",
    "print(\"R2: \", np.round(r2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##시작하는 등고선이 다르기 때문에 다 다른 R2가 나옴 0.396(튜닝전) --> 튜닝 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##하이퍼 파라미터 튜닝 -->최적의 하이퍼 파라미터 찾기 ##\n",
    "#SGDRegressor학습률 \n",
    "help(SGDRegressor)\n",
    "#max_iter 경사를 몇번 반복? (너무 적게 잡으면 최소값에 도달하지 못해도 끝나버림)\n",
    "#tol\n",
    "#random state 하이퍼 파라미터의 효과 알기 위해 시작점 고정\n",
    "#learning rate 학습스케쥴(학습율 - 처음부터 끝까지 같은 보폭으로 내려옴) \n",
    "#eta0 임의의 점에서 학습 스텝의 폭 (첫번째 학습율과 동일하게 내려옴)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.54892591] [0.492519]\n",
      "y = 3.548926X + (0.493)\n",
      "MSE: 59.472\n",
      "RMSE:  7.712\n",
      "R2:  0.351\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# 데이터 수집\n",
    "boston = load_boston()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df =pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "\n",
    "X = pd.DataFrame(df['RM'])\n",
    "y = boston.target\n",
    "\n",
    "# 전체 데이터 중 80%는 학습용, 20%는 검증용으로 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\\\n",
    "                                                   random_state=1)\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "#모델 객체 생성\n",
    "reg = SGDRegressor(max_iter=10000000, eta0 = 0.0001, loss='squared_loss') #튜닝한 부분 \n",
    "\n",
    "# 모델 학습\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# 계수 및 절편 확인: _속성은 학습을 통해 결정되는 속성\n",
    "print(reg.coef_, reg.intercept_)\n",
    "\n",
    "#회귀식\n",
    "print(\"y = {:2f}X + ({:.3f})\".format(reg.coef_[0], reg.intercept_[0])) #이부분은 OLS와 다름 \n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 예측 수행\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# MSE, RMSE, r2_score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"MSE:\", np.round(mse, 3))\n",
    "print(\"RMSE: \", np.round(rmse, 3))\n",
    "print(\"R2: \", np.round(r2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##최적의 기술  -->OLS(정답)의 값과 가까워 지기 위해 \n",
    "#scaling (변수 단위 민감)\n",
    "#Gridsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD with scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.62413807] [21.47096143]\n",
      "y = 5.624138X + (21.471)\n",
      "MSE: 37.667\n",
      "RMSE:  6.137\n",
      "R2:  0.589\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# 데이터 수집\n",
    "boston = load_boston()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df =pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "\n",
    "X = pd.DataFrame(df['RM'])\n",
    "y = boston.target\n",
    "\n",
    "# 전체 데이터 중 80%는 학습용, 20%는 검증용으로 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=1)\n",
    "              \n",
    "#스케일링(표준화)  \n",
    "train_mean = np.mean(X_train, axis=0)\n",
    "train_std = np.std(X_train, axis=0)\n",
    "X_train = (X_train - train_mean)/train_std\n",
    "X_test= (X_test - train_mean)/ train_std\n",
    "\n",
    "#sklearn.preprocessing standardscaler()로하면 직접 스케일링 안해줘도 표준화됨#\n",
    "\n",
    "#from sklearn.preprocessing import StandardScaler()\n",
    "\n",
    "#scaler = StandScaler()\n",
    "#X_train = scaler.fit_transform(x_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "#모델 객체 생성\n",
    "reg = SGDRegressor(max_iter=10000000, eta0 = 0.0001, loss='squared_loss') #튜닝한 부분 \n",
    "##max iter 10000000번 , eta0 학습률 \n",
    "##loss(최소값)<0.0001(학습률)\n",
    "\n",
    "# 모델 학습\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# 계수 및 절편 확인: _속성은 학습을 통해 결정되는 속성\n",
    "print(reg.coef_, reg.intercept_)\n",
    "\n",
    "#회귀식\n",
    "print(\"y = {:2f}X + ({:.3f})\".format(reg.coef_[0], reg.intercept_[0])) #이부분은 OLS와 다름 \n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 예측 수행\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# MSE, RMSE, r2_score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"MSE:\", np.round(mse, 3))\n",
    "print(\"RMSE: \", np.round(rmse, 3))\n",
    "print(\"R2: \", np.round(r2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##R2값이 정답과 가까워짐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##손실함수의 최솟값을 찾는 것은 모델의 최적화된 파라미터를 찾는 것 -> 경사하강법\n",
    "#배치 경사\n",
    "#확률적 경사 \n",
    "#미니 경사 \n",
    "#현재로서, 확률적 경사 구현"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
