{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6293126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7edf4cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "11501568/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99d6b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (X_train/255).reshape(-1,784)\n",
    "X_test = (X_test/255).reshape(-1,784)\n",
    "#X_train.shape\n",
    "#-1:차원을 맞춰서 (,)해주면, 전체 개수에 맞춰서 나머지 차원을 보고 계산해줌(60000,784)와 같은 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cc5b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#생성자 만들기 - 생성자망\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "g = Sequential()\n",
    "g.add(Dense(units=256,input_dim=100,activation=LeakyReLU(0.2)))\n",
    "g.add(Dense(units=512, activation=LeakyReLU(0.2)))\n",
    "g.add(Dense(units=1024, activation=LeakyReLU(0.2)))\n",
    "\n",
    "#units:이미지 데이터 2의n승(숫자 크게-256부터 시작)\n",
    "#noise(random bector)로 가짜데이터 만듬 -> input_dim: 벡터의 feature수(차원수)\n",
    "#0.2 : -1이 들어왔을 때 -1*0.2=-0.2만큼 함수값을 내림 \n",
    "\n",
    "#출력층\n",
    "g.add(Dense(units=784, activation='sigmoid'))\n",
    "#units=내보내고 싶은 데이터 수\n",
    "g.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4347d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#판별자 망 -초기:분류기\n",
    "d = Sequential()\n",
    "d.add(Dense(units=1024,input_dim=784,activation=LeakyReLU(0.2)))\n",
    "#input_dim=784 생성자가 출력층에서 784개의 데이터를 내보냄 \n",
    "#생성자와 반대로 unit개수를 줄여가며 층을 쌓음\n",
    "d.add(Dense(units=512,activation=LeakyReLU(0.2)))\n",
    "d.add(Dense(units=512,activation=LeakyReLU(0.2)))\n",
    "d.add(Dense(units=1,activation='sigmoid')) #출력증\n",
    "#판별자에서 출력값 : 분류값(0,1)-> binary class '가짜다아니다'\n",
    "\n",
    "d.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "d.trainable = False #훈련가능하지 않은 상태로 바꿔줌-> 학습X ->가중치갱신X\n",
    "#판별자는 자신의 가중치를 가지고 있고 가짜데이터가 갖고있는 가중치를 계속 바꿔주지 않음(가짜데이터의 학습을 다시 안함-그저 판별만함)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#노드가 여러개 있을때 점수들을 관리해줄때 -> softmax\n",
    "#binary이므로 sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df5f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#생성자와 판별자 층 쌓을 때 units의 수는 같은 값으로 서로 반대되도록 순서 지정하면 되는건가요?  꼭그런건아님\n",
    "#아니면 생성자와 판별자의 unit의 숫자가 서로 반대되지 않더라도 생성자는 늘리고 판별자는 줄이기만 하면 되는건가요? 대체로 그렇다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a7df82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#생성자와 판별자를 연결한 신경망\n",
    "from keras.models import Model\n",
    "from keras.layers import Input #전체 간 구조에 해당하는 인풋만듬\n",
    "\n",
    "#input층\n",
    "gan_input = Input(shape=(100, ))\n",
    "#만들고자하는 재료의 모양\n",
    "\n",
    "#생성자에게 보내줌\n",
    "x = g(inputs = gan_input)\n",
    "#생성자가 받을 input의 모양\n",
    "\n",
    "#중간결과물 판별자가 받음\n",
    "gan_output = d(x)\n",
    "#이를 받아서 최종적으로 내보냄 \n",
    "\n",
    "\n",
    "#모델 쌓기\n",
    "model = Model(gan_input, gan_output)#(입력,출력)\n",
    "#Model : 알아서 층을 쌓아 모델완성해줌\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#판별자는 0or1 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "561b0659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH= 1 BATCH= 1 G_LOSS= [6.25101900100708, 0.0] D_LOSS= [0.6248681545257568, 0.5]\n",
      "EPOCH= 1 BATCH= 2 G_LOSS= [3.66963791847229, 0.0] D_LOSS= [0.41628673672676086, 0.5]\n",
      "EPOCH= 1 BATCH= 3 G_LOSS= [4.668774127960205, 0.0] D_LOSS= [0.20419344305992126, 0.5]\n",
      "EPOCH= 1 BATCH= 4 G_LOSS= [7.823421478271484, 0.0] D_LOSS= [0.24220320582389832, 0.5]\n",
      "EPOCH= 1 BATCH= 5 G_LOSS= [9.416372299194336, 0.0] D_LOSS= [0.20090064406394958, 0.5]\n",
      "EPOCH= 1 BATCH= 6 G_LOSS= [8.996955871582031, 0.0] D_LOSS= [0.19662490487098694, 0.5]\n",
      "EPOCH= 1 BATCH= 7 G_LOSS= [7.058674335479736, 0.0] D_LOSS= [0.2572683095932007, 0.5]\n",
      "EPOCH= 1 BATCH= 8 G_LOSS= [5.982072830200195, 0.0] D_LOSS= [0.27769196033477783, 0.5]\n",
      "EPOCH= 1 BATCH= 9 G_LOSS= [7.0629119873046875, 0.0] D_LOSS= [0.23666176199913025, 0.5]\n",
      "EPOCH= 1 BATCH= 10 G_LOSS= [7.030662536621094, 0.0] D_LOSS= [0.22547750174999237, 0.5]\n",
      "EPOCH= 1 BATCH= 11 G_LOSS= [8.184053421020508, 0.0] D_LOSS= [0.20659276843070984, 0.5]\n",
      "EPOCH= 1 BATCH= 12 G_LOSS= [9.613183975219727, 0.0] D_LOSS= [0.19215872883796692, 0.5]\n",
      "EPOCH= 1 BATCH= 13 G_LOSS= [9.816547393798828, 0.0] D_LOSS= [0.19757315516471863, 0.5]\n",
      "EPOCH= 1 BATCH= 14 G_LOSS= [8.928888320922852, 0.0] D_LOSS= [0.19118550419807434, 0.5]\n",
      "EPOCH= 1 BATCH= 15 G_LOSS= [8.41113567352295, 0.0] D_LOSS= [0.20706433057785034, 0.5]\n",
      "EPOCH= 1 BATCH= 16 G_LOSS= [8.688071250915527, 0.0] D_LOSS= [0.1935555636882782, 0.5]\n",
      "EPOCH= 1 BATCH= 17 G_LOSS= [8.825660705566406, 0.0] D_LOSS= [0.19251565635204315, 0.5]\n",
      "EPOCH= 1 BATCH= 18 G_LOSS= [8.312121391296387, 0.0] D_LOSS= [0.1846122145652771, 0.5]\n",
      "EPOCH= 1 BATCH= 19 G_LOSS= [8.754438400268555, 0.0] D_LOSS= [0.18952511250972748, 0.5]\n",
      "EPOCH= 1 BATCH= 20 G_LOSS= [9.717164039611816, 0.0] D_LOSS= [0.17828841507434845, 0.5]\n",
      "EPOCH= 1 BATCH= 21 G_LOSS= [9.969353675842285, 0.0] D_LOSS= [0.17704638838768005, 0.5]\n",
      "EPOCH= 1 BATCH= 22 G_LOSS= [9.04052734375, 0.0] D_LOSS= [0.17746976017951965, 0.5]\n",
      "EPOCH= 1 BATCH= 23 G_LOSS= [8.291719436645508, 0.0] D_LOSS= [0.17298707365989685, 0.5]\n",
      "EPOCH= 1 BATCH= 24 G_LOSS= [8.117498397827148, 0.0] D_LOSS= [0.17405983805656433, 0.5]\n",
      "EPOCH= 1 BATCH= 25 G_LOSS= [8.26992130279541, 0.0] D_LOSS= [0.17262893915176392, 0.5]\n",
      "EPOCH= 1 BATCH= 26 G_LOSS= [8.315959930419922, 0.0] D_LOSS= [0.16923999786376953, 0.5]\n",
      "EPOCH= 1 BATCH= 27 G_LOSS= [8.174534797668457, 0.0] D_LOSS= [0.17019568383693695, 0.5]\n",
      "EPOCH= 1 BATCH= 28 G_LOSS= [7.704305171966553, 0.0] D_LOSS= [0.16928496956825256, 0.5]\n",
      "EPOCH= 1 BATCH= 29 G_LOSS= [7.2782487869262695, 0.0] D_LOSS= [0.16785869002342224, 0.5]\n",
      "EPOCH= 1 BATCH= 30 G_LOSS= [7.3172807693481445, 0.0] D_LOSS= [0.16878348588943481, 0.5]\n",
      "EPOCH= 1 BATCH= 31 G_LOSS= [7.504133701324463, 0.0] D_LOSS= [0.1681308150291443, 0.5]\n",
      "EPOCH= 1 BATCH= 32 G_LOSS= [7.687788009643555, 0.0] D_LOSS= [0.16900911927223206, 0.5]\n",
      "EPOCH= 1 BATCH= 33 G_LOSS= [7.7593207359313965, 0.0] D_LOSS= [0.16808566451072693, 0.5]\n",
      "EPOCH= 1 BATCH= 34 G_LOSS= [7.6741943359375, 0.0] D_LOSS= [0.16947756707668304, 0.5]\n",
      "EPOCH= 1 BATCH= 35 G_LOSS= [7.763970375061035, 0.0] D_LOSS= [0.16880252957344055, 0.5]\n",
      "EPOCH= 1 BATCH= 36 G_LOSS= [8.210992813110352, 0.0] D_LOSS= [0.168293297290802, 0.5]\n",
      "EPOCH= 1 BATCH= 37 G_LOSS= [8.674271583557129, 0.0] D_LOSS= [0.16759155690670013, 0.5]\n",
      "EPOCH= 1 BATCH= 38 G_LOSS= [8.945130348205566, 0.0] D_LOSS= [0.1669882833957672, 0.5]\n",
      "EPOCH= 1 BATCH= 39 G_LOSS= [9.004791259765625, 0.0] D_LOSS= [0.16659849882125854, 0.5]\n",
      "EPOCH= 1 BATCH= 40 G_LOSS= [9.078903198242188, 0.0] D_LOSS= [0.16608616709709167, 0.5]\n",
      "EPOCH= 1 BATCH= 41 G_LOSS= [9.291208267211914, 0.0] D_LOSS= [0.16581648588180542, 0.5]\n",
      "EPOCH= 1 BATCH= 42 G_LOSS= [9.583686828613281, 0.0] D_LOSS= [0.16594599187374115, 0.5]\n",
      "EPOCH= 1 BATCH= 43 G_LOSS= [9.777631759643555, 0.0] D_LOSS= [0.16630439460277557, 0.5]\n",
      "EPOCH= 1 BATCH= 44 G_LOSS= [9.836172103881836, 0.0] D_LOSS= [0.16611424088478088, 0.5]\n",
      "EPOCH= 1 BATCH= 45 G_LOSS= [9.846301078796387, 0.0] D_LOSS= [0.16613037884235382, 0.5]\n",
      "EPOCH= 1 BATCH= 46 G_LOSS= [9.892248153686523, 0.0] D_LOSS= [0.16549715399742126, 0.5]\n",
      "EPOCH= 1 BATCH= 47 G_LOSS= [9.97270393371582, 0.0] D_LOSS= [0.1659906804561615, 0.5]\n",
      "EPOCH= 1 BATCH= 48 G_LOSS= [10.078049659729004, 0.0] D_LOSS= [0.16498568654060364, 0.5]\n",
      "EPOCH= 1 BATCH= 49 G_LOSS= [10.105073928833008, 0.0] D_LOSS= [0.1648528277873993, 0.5]\n",
      "EPOCH= 1 BATCH= 50 G_LOSS= [10.12237548828125, 0.0] D_LOSS= [0.1654639095067978, 0.5]\n",
      "EPOCH= 1 BATCH= 51 G_LOSS= [10.10157585144043, 0.0] D_LOSS= [0.16541123390197754, 0.5]\n",
      "EPOCH= 1 BATCH= 52 G_LOSS= [10.15119743347168, 0.0] D_LOSS= [0.1650259643793106, 0.5]\n",
      "EPOCH= 1 BATCH= 53 G_LOSS= [10.20570182800293, 0.0] D_LOSS= [0.16512280702590942, 0.5]\n",
      "EPOCH= 1 BATCH= 54 G_LOSS= [10.254378318786621, 0.0] D_LOSS= [0.16533808410167694, 0.5]\n",
      "EPOCH= 1 BATCH= 55 G_LOSS= [10.247054100036621, 0.0] D_LOSS= [0.1643587201833725, 0.5]\n",
      "EPOCH= 1 BATCH= 56 G_LOSS= [10.303237915039062, 0.0] D_LOSS= [0.16495588421821594, 0.5]\n",
      "EPOCH= 1 BATCH= 57 G_LOSS= [10.354604721069336, 0.0] D_LOSS= [0.1647127866744995, 0.5]\n",
      "EPOCH= 1 BATCH= 58 G_LOSS= [10.311152458190918, 0.0] D_LOSS= [0.1644275039434433, 0.5]\n",
      "EPOCH= 1 BATCH= 59 G_LOSS= [10.28199291229248, 0.0] D_LOSS= [0.16383889317512512, 0.5]\n",
      "EPOCH= 1 BATCH= 60 G_LOSS= [10.315656661987305, 0.0] D_LOSS= [0.16440750658512115, 0.5]\n",
      "EPOCH= 1 BATCH= 61 G_LOSS= [10.448551177978516, 0.0] D_LOSS= [0.16430765390396118, 0.5]\n",
      "EPOCH= 1 BATCH= 62 G_LOSS= [10.436298370361328, 0.0] D_LOSS= [0.16440998017787933, 0.5]\n",
      "EPOCH= 1 BATCH= 63 G_LOSS= [10.321863174438477, 0.0] D_LOSS= [0.16425183415412903, 0.5]\n",
      "EPOCH= 1 BATCH= 64 G_LOSS= [10.22010326385498, 0.0] D_LOSS= [0.16404223442077637, 0.5]\n",
      "EPOCH= 1 BATCH= 65 G_LOSS= [10.337316513061523, 0.0] D_LOSS= [0.16425926983356476, 0.5]\n",
      "EPOCH= 1 BATCH= 66 G_LOSS= [10.455392837524414, 0.0] D_LOSS= [0.16425514221191406, 0.5]\n",
      "EPOCH= 1 BATCH= 67 G_LOSS= [10.379484176635742, 0.0] D_LOSS= [0.1641792207956314, 0.5]\n",
      "EPOCH= 1 BATCH= 68 G_LOSS= [10.196046829223633, 0.0] D_LOSS= [0.16420134902000427, 0.5]\n",
      "EPOCH= 1 BATCH= 69 G_LOSS= [10.153044700622559, 0.0] D_LOSS= [0.1636584997177124, 0.5]\n",
      "EPOCH= 1 BATCH= 70 G_LOSS= [10.34080696105957, 0.0] D_LOSS= [0.16441597044467926, 0.5]\n",
      "EPOCH= 1 BATCH= 71 G_LOSS= [10.450170516967773, 0.0] D_LOSS= [0.16381031274795532, 0.5]\n",
      "EPOCH= 1 BATCH= 72 G_LOSS= [10.289958000183105, 0.0] D_LOSS= [0.16420844197273254, 0.5]\n",
      "EPOCH= 1 BATCH= 73 G_LOSS= [10.154831886291504, 0.0] D_LOSS= [0.1637141853570938, 0.5]\n",
      "EPOCH= 1 BATCH= 74 G_LOSS= [10.089204788208008, 0.0] D_LOSS= [0.16403263807296753, 0.5]\n",
      "EPOCH= 1 BATCH= 75 G_LOSS= [10.210027694702148, 0.0] D_LOSS= [0.1643722802400589, 0.5]\n",
      "EPOCH= 1 BATCH= 76 G_LOSS= [10.404834747314453, 0.0] D_LOSS= [0.16430708765983582, 0.5]\n",
      "EPOCH= 1 BATCH= 77 G_LOSS= [10.47553825378418, 0.0] D_LOSS= [0.16388818621635437, 0.5]\n",
      "EPOCH= 1 BATCH= 78 G_LOSS= [10.200606346130371, 0.0] D_LOSS= [0.16445541381835938, 0.5]\n",
      "EPOCH= 1 BATCH= 79 G_LOSS= [10.075984001159668, 0.0] D_LOSS= [0.16348165273666382, 0.5]\n",
      "EPOCH= 1 BATCH= 80 G_LOSS= [10.353372573852539, 0.0] D_LOSS= [0.16433963179588318, 0.5]\n",
      "EPOCH= 1 BATCH= 81 G_LOSS= [10.571109771728516, 0.0] D_LOSS= [0.16347214579582214, 0.5]\n",
      "EPOCH= 1 BATCH= 82 G_LOSS= [10.402831077575684, 0.0] D_LOSS= [0.16404777765274048, 0.5]\n",
      "EPOCH= 1 BATCH= 83 G_LOSS= [10.147638320922852, 0.0] D_LOSS= [0.16356858611106873, 0.5]\n",
      "EPOCH= 1 BATCH= 84 G_LOSS= [10.09910774230957, 0.0] D_LOSS= [0.16389787197113037, 0.5]\n",
      "EPOCH= 1 BATCH= 85 G_LOSS= [10.455536842346191, 0.0] D_LOSS= [0.1642594039440155, 0.5]\n",
      "EPOCH= 1 BATCH= 86 G_LOSS= [10.67870044708252, 0.0] D_LOSS= [0.16336524486541748, 0.5]\n",
      "EPOCH= 1 BATCH= 87 G_LOSS= [10.450515747070312, 0.0] D_LOSS= [0.16427089273929596, 0.5]\n",
      "EPOCH= 1 BATCH= 88 G_LOSS= [10.218755722045898, 0.0] D_LOSS= [0.16328930854797363, 0.5]\n",
      "EPOCH= 1 BATCH= 89 G_LOSS= [10.265274047851562, 0.0] D_LOSS= [0.16378936171531677, 0.5]\n",
      "EPOCH= 1 BATCH= 90 G_LOSS= [10.516386032104492, 0.0] D_LOSS= [0.1635431945323944, 0.5]\n",
      "EPOCH= 1 BATCH= 91 G_LOSS= [10.582213401794434, 0.0] D_LOSS= [0.16368119418621063, 0.5]\n",
      "EPOCH= 1 BATCH= 92 G_LOSS= [10.440232276916504, 0.0] D_LOSS= [0.1635296642780304, 0.5]\n",
      "EPOCH= 1 BATCH= 93 G_LOSS= [10.286676406860352, 0.0] D_LOSS= [0.1633748561143875, 0.5]\n",
      "EPOCH= 1 BATCH= 94 G_LOSS= [10.458569526672363, 0.0] D_LOSS= [0.16375526785850525, 0.5]\n",
      "EPOCH= 1 BATCH= 95 G_LOSS= [10.606595993041992, 0.0] D_LOSS= [0.1633777618408203, 0.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH= 1 BATCH= 96 G_LOSS= [10.518138885498047, 0.0] D_LOSS= [0.1635397970676422, 0.5]\n",
      "EPOCH= 1 BATCH= 97 G_LOSS= [10.39091968536377, 0.0] D_LOSS= [0.16333939135074615, 0.5]\n",
      "EPOCH= 1 BATCH= 98 G_LOSS= [10.415704727172852, 0.0] D_LOSS= [0.16322818398475647, 0.5]\n",
      "EPOCH= 1 BATCH= 99 G_LOSS= [10.566773414611816, 0.0] D_LOSS= [0.16310523450374603, 0.5]\n",
      "EPOCH= 1 BATCH= 100 G_LOSS= [10.60588264465332, 0.0] D_LOSS= [0.163274347782135, 0.5]\n",
      "EPOCH= 1 BATCH= 101 G_LOSS= [10.505582809448242, 0.0] D_LOSS= [0.16322200000286102, 0.5]\n",
      "EPOCH= 1 BATCH= 102 G_LOSS= [10.453165054321289, 0.0] D_LOSS= [0.16316142678260803, 0.5]\n",
      "EPOCH= 1 BATCH= 103 G_LOSS= [10.597972869873047, 0.0] D_LOSS= [0.16323992609977722, 0.5]\n",
      "EPOCH= 1 BATCH= 104 G_LOSS= [10.671052932739258, 0.0] D_LOSS= [0.1632726937532425, 0.5]\n",
      "EPOCH= 1 BATCH= 105 G_LOSS= [10.572541236877441, 0.0] D_LOSS= [0.16314655542373657, 0.5]\n",
      "EPOCH= 1 BATCH= 106 G_LOSS= [10.526443481445312, 0.0] D_LOSS= [0.1631225049495697, 0.5]\n",
      "EPOCH= 1 BATCH= 107 G_LOSS= [10.580443382263184, 0.0] D_LOSS= [0.16314342617988586, 0.5]\n",
      "EPOCH= 1 BATCH= 108 G_LOSS= [10.737837791442871, 0.0] D_LOSS= [0.1631486564874649, 0.5]\n",
      "EPOCH= 1 BATCH= 109 G_LOSS= [10.683930397033691, 0.0] D_LOSS= [0.1632900834083557, 0.5]\n",
      "EPOCH= 1 BATCH= 110 G_LOSS= [10.566099166870117, 0.0] D_LOSS= [0.16301751136779785, 0.5]\n",
      "EPOCH= 1 BATCH= 111 G_LOSS= [10.623946189880371, 0.0] D_LOSS= [0.16310016810894012, 0.5]\n",
      "EPOCH= 1 BATCH= 112 G_LOSS= [10.808897972106934, 0.0] D_LOSS= [0.16303828358650208, 0.5]\n",
      "EPOCH= 1 BATCH= 113 G_LOSS= [10.848875045776367, 0.0] D_LOSS= [0.16295959055423737, 0.5]\n",
      "EPOCH= 1 BATCH= 114 G_LOSS= [10.728418350219727, 0.0] D_LOSS= [0.16311195492744446, 0.5]\n",
      "EPOCH= 1 BATCH= 115 G_LOSS= [10.642183303833008, 0.0] D_LOSS= [0.16288632154464722, 0.5]\n",
      "EPOCH= 1 BATCH= 116 G_LOSS= [10.815385818481445, 0.0] D_LOSS= [0.16339579224586487, 0.5]\n",
      "EPOCH= 1 BATCH= 117 G_LOSS= [10.93427562713623, 0.0] D_LOSS= [0.16299571096897125, 0.5]\n",
      "EPOCH= 1 BATCH= 118 G_LOSS= [10.770186424255371, 0.0] D_LOSS= [0.16317638754844666, 0.5]\n",
      "EPOCH= 1 BATCH= 119 G_LOSS= [10.744104385375977, 0.0] D_LOSS= [0.1632535457611084, 0.5]\n",
      "EPOCH= 1 BATCH= 120 G_LOSS= [10.898042678833008, 0.0] D_LOSS= [0.16316109895706177, 0.5]\n",
      "EPOCH= 1 BATCH= 121 G_LOSS= [10.983827590942383, 0.0] D_LOSS= [0.16298097372055054, 0.5]\n",
      "EPOCH= 1 BATCH= 122 G_LOSS= [10.80988883972168, 0.0] D_LOSS= [0.16313475370407104, 0.5]\n",
      "EPOCH= 1 BATCH= 123 G_LOSS= [10.769124031066895, 0.0] D_LOSS= [0.16296814382076263, 0.5]\n",
      "EPOCH= 1 BATCH= 124 G_LOSS= [10.94913101196289, 0.0] D_LOSS= [0.16310247778892517, 0.5]\n",
      "EPOCH= 1 BATCH= 125 G_LOSS= [11.027511596679688, 0.0] D_LOSS= [0.1630634069442749, 0.5]\n",
      "EPOCH= 1 BATCH= 126 G_LOSS= [10.898271560668945, 0.0] D_LOSS= [0.16324132680892944, 0.5]\n",
      "EPOCH= 1 BATCH= 127 G_LOSS= [10.798050880432129, 0.0] D_LOSS= [0.16288819909095764, 0.5]\n",
      "EPOCH= 1 BATCH= 128 G_LOSS= [10.947661399841309, 0.0] D_LOSS= [0.16311344504356384, 0.5]\n",
      "EPOCH= 1 BATCH= 129 G_LOSS= [11.225096702575684, 0.0] D_LOSS= [0.16297149658203125, 0.5]\n",
      "EPOCH= 1 BATCH= 130 G_LOSS= [11.140832901000977, 0.0] D_LOSS= [0.16341370344161987, 0.5]\n",
      "EPOCH= 1 BATCH= 131 G_LOSS= [10.944657325744629, 0.0] D_LOSS= [0.16297905147075653, 0.5]\n",
      "EPOCH= 1 BATCH= 132 G_LOSS= [11.000014305114746, 0.0] D_LOSS= [0.16312114894390106, 0.5]\n",
      "EPOCH= 1 BATCH= 133 G_LOSS= [11.213377952575684, 0.0] D_LOSS= [0.16307634115219116, 0.5]\n",
      "EPOCH= 1 BATCH= 134 G_LOSS= [11.343259811401367, 0.0] D_LOSS= [0.16292142868041992, 0.5]\n",
      "EPOCH= 1 BATCH= 135 G_LOSS= [11.19990348815918, 0.0] D_LOSS= [0.16312575340270996, 0.5]\n",
      "EPOCH= 1 BATCH= 136 G_LOSS= [11.131166458129883, 0.0] D_LOSS= [0.1629033088684082, 0.5]\n",
      "EPOCH= 1 BATCH= 137 G_LOSS= [11.285720825195312, 0.0] D_LOSS= [0.163196861743927, 0.5]\n",
      "EPOCH= 1 BATCH= 138 G_LOSS= [11.401320457458496, 0.0] D_LOSS= [0.16302502155303955, 0.5]\n",
      "EPOCH= 1 BATCH= 139 G_LOSS= [11.309968948364258, 0.0] D_LOSS= [0.16300290822982788, 0.5]\n",
      "EPOCH= 1 BATCH= 140 G_LOSS= [11.217466354370117, 0.0] D_LOSS= [0.16295501589775085, 0.5]\n",
      "EPOCH= 1 BATCH= 141 G_LOSS= [11.298406600952148, 0.0] D_LOSS= [0.16305115818977356, 0.5]\n",
      "EPOCH= 1 BATCH= 142 G_LOSS= [11.450074195861816, 0.0] D_LOSS= [0.1628970056772232, 0.5]\n",
      "EPOCH= 1 BATCH= 143 G_LOSS= [11.39989185333252, 0.0] D_LOSS= [0.16296303272247314, 0.5]\n",
      "EPOCH= 1 BATCH= 144 G_LOSS= [11.283727645874023, 0.0] D_LOSS= [0.16282975673675537, 0.5]\n",
      "EPOCH= 1 BATCH= 145 G_LOSS= [11.357526779174805, 0.0] D_LOSS= [0.16294145584106445, 0.5]\n",
      "EPOCH= 1 BATCH= 146 G_LOSS= [11.433333396911621, 0.0] D_LOSS= [0.16289378702640533, 0.5]\n",
      "EPOCH= 1 BATCH= 147 G_LOSS= [11.453913688659668, 0.0] D_LOSS= [0.16288474202156067, 0.5]\n",
      "EPOCH= 1 BATCH= 148 G_LOSS= [11.352009773254395, 0.0] D_LOSS= [0.16293078660964966, 0.5]\n",
      "EPOCH= 1 BATCH= 149 G_LOSS= [11.33771800994873, 0.0] D_LOSS= [0.1628197431564331, 0.5]\n",
      "EPOCH= 1 BATCH= 150 G_LOSS= [11.473665237426758, 0.0] D_LOSS= [0.16298028826713562, 0.5]\n",
      "EPOCH= 1 BATCH= 151 G_LOSS= [11.510177612304688, 0.0] D_LOSS= [0.16292910277843475, 0.5]\n",
      "EPOCH= 1 BATCH= 152 G_LOSS= [11.421392440795898, 0.0] D_LOSS= [0.16301517188549042, 0.5]\n",
      "EPOCH= 1 BATCH= 153 G_LOSS= [11.426807403564453, 0.0] D_LOSS= [0.162789985537529, 0.5]\n",
      "EPOCH= 1 BATCH= 154 G_LOSS= [11.506439208984375, 0.0] D_LOSS= [0.16302111744880676, 0.5]\n",
      "EPOCH= 1 BATCH= 155 G_LOSS= [11.493902206420898, 0.0] D_LOSS= [0.1628543585538864, 0.5]\n",
      "EPOCH= 1 BATCH= 156 G_LOSS= [11.438867568969727, 0.0] D_LOSS= [0.16289545595645905, 0.5]\n",
      "EPOCH= 1 BATCH= 157 G_LOSS= [11.497233390808105, 0.0] D_LOSS= [0.16285976767539978, 0.5]\n",
      "EPOCH= 1 BATCH= 158 G_LOSS= [11.497663497924805, 0.0] D_LOSS= [0.1628642976284027, 0.5]\n",
      "EPOCH= 1 BATCH= 159 G_LOSS= [11.498427391052246, 0.0] D_LOSS= [0.16290470957756042, 0.5]\n",
      "EPOCH= 1 BATCH= 160 G_LOSS= [11.557977676391602, 0.0] D_LOSS= [0.16287142038345337, 0.5]\n",
      "EPOCH= 1 BATCH= 161 G_LOSS= [11.531815528869629, 0.0] D_LOSS= [0.1629171222448349, 0.5]\n",
      "EPOCH= 1 BATCH= 162 G_LOSS= [11.541030883789062, 0.0] D_LOSS= [0.16282281279563904, 0.5]\n",
      "EPOCH= 1 BATCH= 163 G_LOSS= [11.527546882629395, 0.0] D_LOSS= [0.16286949813365936, 0.5]\n",
      "EPOCH= 1 BATCH= 164 G_LOSS= [11.52503776550293, 0.0] D_LOSS= [0.16286338865756989, 0.5]\n",
      "EPOCH= 1 BATCH= 165 G_LOSS= [11.589478492736816, 0.0] D_LOSS= [0.1628350019454956, 0.5]\n",
      "EPOCH= 1 BATCH= 166 G_LOSS= [11.61772632598877, 0.0] D_LOSS= [0.16283081471920013, 0.5]\n",
      "EPOCH= 1 BATCH= 167 G_LOSS= [11.504332542419434, 0.0] D_LOSS= [0.1629043072462082, 0.5]\n",
      "EPOCH= 1 BATCH= 168 G_LOSS= [11.517072677612305, 0.0] D_LOSS= [0.1628677397966385, 0.5]\n",
      "EPOCH= 1 BATCH= 169 G_LOSS= [11.643980026245117, 0.0] D_LOSS= [0.1628662645816803, 0.5]\n",
      "EPOCH= 1 BATCH= 170 G_LOSS= [11.607547760009766, 0.0] D_LOSS= [0.16294091939926147, 0.5]\n",
      "EPOCH= 1 BATCH= 171 G_LOSS= [11.513383865356445, 0.0] D_LOSS= [0.16278399527072906, 0.5]\n",
      "EPOCH= 1 BATCH= 172 G_LOSS= [11.55953598022461, 0.0] D_LOSS= [0.1628655344247818, 0.5]\n",
      "EPOCH= 1 BATCH= 173 G_LOSS= [11.598421096801758, 0.0] D_LOSS= [0.16282466053962708, 0.5]\n",
      "EPOCH= 1 BATCH= 174 G_LOSS= [11.569275856018066, 0.0] D_LOSS= [0.16297078132629395, 0.5]\n",
      "EPOCH= 1 BATCH= 175 G_LOSS= [11.530624389648438, 0.0] D_LOSS= [0.16280479729175568, 0.5]\n",
      "EPOCH= 1 BATCH= 176 G_LOSS= [11.558614730834961, 0.0] D_LOSS= [0.16280734539031982, 0.5]\n",
      "EPOCH= 1 BATCH= 177 G_LOSS= [11.563791275024414, 0.0] D_LOSS= [0.1628282070159912, 0.5]\n",
      "EPOCH= 1 BATCH= 178 G_LOSS= [11.571134567260742, 0.0] D_LOSS= [0.1627723127603531, 0.5]\n",
      "EPOCH= 1 BATCH= 179 G_LOSS= [11.535242080688477, 0.0] D_LOSS= [0.16281121969223022, 0.5]\n",
      "EPOCH= 1 BATCH= 180 G_LOSS= [11.517454147338867, 0.0] D_LOSS= [0.16277018189430237, 0.5]\n",
      "EPOCH= 1 BATCH= 181 G_LOSS= [11.52206802368164, 0.0] D_LOSS= [0.1627904772758484, 0.5]\n",
      "EPOCH= 1 BATCH= 182 G_LOSS= [11.599502563476562, 0.0] D_LOSS= [0.16285032033920288, 0.5]\n",
      "EPOCH= 1 BATCH= 183 G_LOSS= [11.599855422973633, 0.0] D_LOSS= [0.16281074285507202, 0.5]\n",
      "EPOCH= 1 BATCH= 184 G_LOSS= [11.493402481079102, 0.0] D_LOSS= [0.16285943984985352, 0.5]\n",
      "EPOCH= 1 BATCH= 185 G_LOSS= [11.554622650146484, 0.0] D_LOSS= [0.16279299557209015, 0.5]\n",
      "EPOCH= 1 BATCH= 186 G_LOSS= [11.659072875976562, 0.0] D_LOSS= [0.16276578605175018, 0.5]\n",
      "EPOCH= 1 BATCH= 187 G_LOSS= [11.571476936340332, 0.0] D_LOSS= [0.1628335416316986, 0.5]\n",
      "EPOCH= 1 BATCH= 188 G_LOSS= [11.553359985351562, 0.0] D_LOSS= [0.16282056272029877, 0.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH= 1 BATCH= 189 G_LOSS= [11.67892074584961, 0.0] D_LOSS= [0.16280508041381836, 0.5]\n",
      "EPOCH= 1 BATCH= 190 G_LOSS= [11.738187789916992, 0.0] D_LOSS= [0.16287720203399658, 0.5]\n",
      "EPOCH= 1 BATCH= 191 G_LOSS= [11.603071212768555, 0.0] D_LOSS= [0.1628602147102356, 0.5]\n",
      "EPOCH= 1 BATCH= 192 G_LOSS= [11.668560981750488, 0.0] D_LOSS= [0.16281428933143616, 0.5]\n",
      "EPOCH= 1 BATCH= 193 G_LOSS= [11.735733032226562, 0.0] D_LOSS= [0.16283603012561798, 0.5]\n",
      "EPOCH= 1 BATCH= 194 G_LOSS= [11.7344970703125, 0.0] D_LOSS= [0.16277453303337097, 0.5]\n",
      "EPOCH= 1 BATCH= 195 G_LOSS= [11.72941780090332, 0.0] D_LOSS= [0.16274619102478027, 0.5]\n",
      "EPOCH= 1 BATCH= 196 G_LOSS= [11.753137588500977, 0.0] D_LOSS= [0.16275939345359802, 0.5]\n",
      "EPOCH= 1 BATCH= 197 G_LOSS= [11.739486694335938, 0.0] D_LOSS= [0.1627899408340454, 0.5]\n",
      "EPOCH= 1 BATCH= 198 G_LOSS= [11.813899040222168, 0.0] D_LOSS= [0.16279244422912598, 0.5]\n",
      "EPOCH= 1 BATCH= 199 G_LOSS= [11.728631973266602, 0.0] D_LOSS= [0.16281552612781525, 0.5]\n",
      "EPOCH= 1 BATCH= 200 G_LOSS= [11.706466674804688, 0.0] D_LOSS= [0.1627892255783081, 0.5]\n",
      "EPOCH= 1 BATCH= 201 G_LOSS= [11.888459205627441, 0.0] D_LOSS= [0.1628578007221222, 0.5]\n",
      "EPOCH= 1 BATCH= 202 G_LOSS= [11.740875244140625, 0.0] D_LOSS= [0.16294646263122559, 0.5]\n",
      "EPOCH= 1 BATCH= 203 G_LOSS= [11.746917724609375, 0.0] D_LOSS= [0.1628199815750122, 0.5]\n",
      "EPOCH= 1 BATCH= 204 G_LOSS= [11.871870040893555, 0.0] D_LOSS= [0.16277703642845154, 0.5]\n",
      "EPOCH= 1 BATCH= 205 G_LOSS= [11.824081420898438, 0.0] D_LOSS= [0.16285717487335205, 0.5]\n",
      "EPOCH= 1 BATCH= 206 G_LOSS= [11.692636489868164, 0.0] D_LOSS= [0.16278545558452606, 0.5]\n",
      "EPOCH= 1 BATCH= 207 G_LOSS= [11.877313613891602, 0.0] D_LOSS= [0.16294513642787933, 0.5]\n",
      "EPOCH= 1 BATCH= 208 G_LOSS= [11.904367446899414, 0.0] D_LOSS= [0.16279539465904236, 0.5]\n",
      "EPOCH= 1 BATCH= 209 G_LOSS= [11.672300338745117, 0.0] D_LOSS= [0.16294293105602264, 0.5]\n",
      "EPOCH= 1 BATCH= 210 G_LOSS= [11.787956237792969, 0.0] D_LOSS= [0.16295677423477173, 0.5]\n",
      "EPOCH= 1 BATCH= 211 G_LOSS= [11.949756622314453, 0.0] D_LOSS= [0.16278746724128723, 0.5]\n",
      "EPOCH= 1 BATCH= 212 G_LOSS= [11.767854690551758, 0.0] D_LOSS= [0.16303715109825134, 0.5]\n",
      "EPOCH= 1 BATCH= 213 G_LOSS= [11.7216796875, 0.0] D_LOSS= [0.16273456811904907, 0.5]\n",
      "EPOCH= 1 BATCH= 214 G_LOSS= [11.963884353637695, 0.0] D_LOSS= [0.16286234557628632, 0.5]\n",
      "EPOCH= 1 BATCH= 215 G_LOSS= [11.892701148986816, 0.0] D_LOSS= [0.1629321426153183, 0.5]\n",
      "EPOCH= 1 BATCH= 216 G_LOSS= [11.730743408203125, 0.0] D_LOSS= [0.16276100277900696, 0.5]\n",
      "EPOCH= 1 BATCH= 217 G_LOSS= [11.83696174621582, 0.0] D_LOSS= [0.16292110085487366, 0.5]\n",
      "EPOCH= 1 BATCH= 218 G_LOSS= [12.008682250976562, 0.0] D_LOSS= [0.16279374063014984, 0.5]\n",
      "EPOCH= 1 BATCH= 219 G_LOSS= [11.963912963867188, 0.0] D_LOSS= [0.16284111142158508, 0.5]\n",
      "EPOCH= 1 BATCH= 220 G_LOSS= [11.806238174438477, 0.0] D_LOSS= [0.16282039880752563, 0.5]\n",
      "EPOCH= 1 BATCH= 221 G_LOSS= [11.974380493164062, 0.0] D_LOSS= [0.16289912164211273, 0.5]\n",
      "EPOCH= 1 BATCH= 222 G_LOSS= [12.06064224243164, 0.0] D_LOSS= [0.1627427041530609, 0.5]\n",
      "EPOCH= 1 BATCH= 223 G_LOSS= [11.952896118164062, 0.0] D_LOSS= [0.16289488971233368, 0.5]\n",
      "EPOCH= 1 BATCH= 224 G_LOSS= [11.938678741455078, 0.0] D_LOSS= [0.16282875835895538, 0.5]\n",
      "EPOCH= 1 BATCH= 225 G_LOSS= [12.133472442626953, 0.0] D_LOSS= [0.16282609105110168, 0.5]\n",
      "EPOCH= 1 BATCH= 226 G_LOSS= [12.036914825439453, 0.0] D_LOSS= [0.1629883050918579, 0.5]\n",
      "EPOCH= 1 BATCH= 227 G_LOSS= [11.963850021362305, 0.0] D_LOSS= [0.16281262040138245, 0.5]\n",
      "EPOCH= 1 BATCH= 228 G_LOSS= [12.09811782836914, 0.0] D_LOSS= [0.16282904148101807, 0.5]\n",
      "EPOCH= 1 BATCH= 229 G_LOSS= [12.16421127319336, 0.0] D_LOSS= [0.16274866461753845, 0.5]\n",
      "EPOCH= 1 BATCH= 230 G_LOSS= [12.014634132385254, 0.0] D_LOSS= [0.1627647578716278, 0.5]\n",
      "EPOCH= 1 BATCH= 231 G_LOSS= [12.066034317016602, 0.0] D_LOSS= [0.16280397772789001, 0.5]\n",
      "EPOCH= 1 BATCH= 232 G_LOSS= [12.205028533935547, 0.0] D_LOSS= [0.16282643377780914, 0.5]\n",
      "EPOCH= 1 BATCH= 233 G_LOSS= [12.084396362304688, 0.0] D_LOSS= [0.1628691852092743, 0.5]\n",
      "EPOCH= 1 BATCH= 234 G_LOSS= [12.034622192382812, 0.0] D_LOSS= [0.162721186876297, 0.5]\n",
      "EPOCH= 1 BATCH= 235 G_LOSS= [12.20887565612793, 0.0] D_LOSS= [0.162826269865036, 0.5]\n",
      "EPOCH= 1 BATCH= 236 G_LOSS= [12.200960159301758, 0.0] D_LOSS= [0.1627504527568817, 0.5]\n",
      "EPOCH= 1 BATCH= 237 G_LOSS= [12.021942138671875, 0.0] D_LOSS= [0.1628362387418747, 0.5]\n",
      "EPOCH= 1 BATCH= 238 G_LOSS= [12.138280868530273, 0.0] D_LOSS= [0.16287577152252197, 0.5]\n",
      "EPOCH= 1 BATCH= 239 G_LOSS= [12.243276596069336, 0.0] D_LOSS= [0.1627102941274643, 0.5]\n",
      "EPOCH= 1 BATCH= 240 G_LOSS= [12.08405590057373, 0.0] D_LOSS= [0.16281543672084808, 0.5]\n",
      "EPOCH= 1 BATCH= 241 G_LOSS= [12.095396995544434, 0.0] D_LOSS= [0.16277310252189636, 0.5]\n",
      "EPOCH= 1 BATCH= 242 G_LOSS= [12.175312042236328, 0.0] D_LOSS= [0.1627139449119568, 0.5]\n",
      "EPOCH= 1 BATCH= 243 G_LOSS= [12.165882110595703, 0.0] D_LOSS= [0.16274352371692657, 0.5]\n",
      "EPOCH= 1 BATCH= 244 G_LOSS= [12.079019546508789, 0.0] D_LOSS= [0.16271600127220154, 0.5]\n",
      "EPOCH= 1 BATCH= 245 G_LOSS= [11.742230415344238, 0.0] D_LOSS= [0.1627158522605896, 0.5]\n",
      "EPOCH= 1 BATCH= 246 G_LOSS= [11.2261962890625, 0.0] D_LOSS= [0.16270814836025238, 0.5]\n",
      "EPOCH= 1 BATCH= 247 G_LOSS= [11.189966201782227, 0.0] D_LOSS= [0.16276687383651733, 0.5]\n",
      "EPOCH= 1 BATCH= 248 G_LOSS= [11.150238990783691, 0.0] D_LOSS= [0.16273991763591766, 0.5]\n",
      "EPOCH= 1 BATCH= 249 G_LOSS= [11.158552169799805, 0.0] D_LOSS= [0.1627381592988968, 0.5]\n",
      "EPOCH= 1 BATCH= 250 G_LOSS= [11.240610122680664, 0.0] D_LOSS= [0.1627236157655716, 0.5]\n",
      "EPOCH= 1 BATCH= 251 G_LOSS= [11.20706844329834, 0.0] D_LOSS= [0.16270031034946442, 0.5]\n",
      "EPOCH= 1 BATCH= 252 G_LOSS= [11.160806655883789, 0.0] D_LOSS= [0.16272728145122528, 0.5]\n",
      "EPOCH= 1 BATCH= 253 G_LOSS= [11.256733894348145, 0.0] D_LOSS= [0.16280287504196167, 0.5]\n",
      "EPOCH= 1 BATCH= 254 G_LOSS= [11.293790817260742, 0.0] D_LOSS= [0.16270142793655396, 0.5]\n",
      "EPOCH= 1 BATCH= 255 G_LOSS= [11.221867561340332, 0.0] D_LOSS= [0.16276168823242188, 0.5]\n",
      "EPOCH= 1 BATCH= 256 G_LOSS= [11.320475578308105, 0.0] D_LOSS= [0.16275858879089355, 0.5]\n",
      "EPOCH= 1 BATCH= 257 G_LOSS= [11.41663932800293, 0.0] D_LOSS= [0.16271910071372986, 0.5]\n",
      "EPOCH= 1 BATCH= 258 G_LOSS= [11.303706169128418, 0.0] D_LOSS= [0.16274362802505493, 0.5]\n",
      "EPOCH= 1 BATCH= 259 G_LOSS= [11.382638931274414, 0.0] D_LOSS= [0.16274848580360413, 0.5]\n",
      "EPOCH= 1 BATCH= 260 G_LOSS= [11.551782608032227, 0.0] D_LOSS= [0.16273736953735352, 0.5]\n",
      "EPOCH= 1 BATCH= 261 G_LOSS= [11.459009170532227, 0.0] D_LOSS= [0.1627773642539978, 0.5]\n",
      "EPOCH= 1 BATCH= 262 G_LOSS= [11.442885398864746, 0.0] D_LOSS= [0.1627086102962494, 0.5]\n",
      "EPOCH= 1 BATCH= 263 G_LOSS= [11.670894622802734, 0.0] D_LOSS= [0.1627826988697052, 0.5]\n",
      "EPOCH= 1 BATCH= 264 G_LOSS= [11.609487533569336, 0.0] D_LOSS= [0.16284702718257904, 0.5]\n",
      "EPOCH= 1 BATCH= 265 G_LOSS= [11.575666427612305, 0.0] D_LOSS= [0.16270951926708221, 0.5]\n",
      "EPOCH= 1 BATCH= 266 G_LOSS= [11.726821899414062, 0.0] D_LOSS= [0.16272492706775665, 0.5]\n",
      "EPOCH= 1 BATCH= 267 G_LOSS= [11.817098617553711, 0.0] D_LOSS= [0.162708580493927, 0.5]\n",
      "EPOCH= 1 BATCH= 268 G_LOSS= [11.731175422668457, 0.0] D_LOSS= [0.16270682215690613, 0.5]\n",
      "EPOCH= 1 BATCH= 269 G_LOSS= [11.79178524017334, 0.0] D_LOSS= [0.1627652645111084, 0.5]\n",
      "EPOCH= 1 BATCH= 270 G_LOSS= [11.851943969726562, 0.0] D_LOSS= [0.16273528337478638, 0.5]\n",
      "EPOCH= 1 BATCH= 271 G_LOSS= [11.774444580078125, 0.0] D_LOSS= [0.16268786787986755, 0.5]\n",
      "EPOCH= 1 BATCH= 272 G_LOSS= [11.837400436401367, 0.0] D_LOSS= [0.1627388894557953, 0.5]\n",
      "EPOCH= 1 BATCH= 273 G_LOSS= [12.016359329223633, 0.0] D_LOSS= [0.16275456547737122, 0.5]\n",
      "EPOCH= 1 BATCH= 274 G_LOSS= [11.855112075805664, 0.0] D_LOSS= [0.16286595165729523, 0.5]\n",
      "EPOCH= 1 BATCH= 275 G_LOSS= [11.867387771606445, 0.0] D_LOSS= [0.16274049878120422, 0.5]\n",
      "EPOCH= 1 BATCH= 276 G_LOSS= [11.99539566040039, 0.0] D_LOSS= [0.1626884937286377, 0.5]\n",
      "EPOCH= 1 BATCH= 277 G_LOSS= [12.000807762145996, 0.0] D_LOSS= [0.16274333000183105, 0.5]\n",
      "EPOCH= 1 BATCH= 278 G_LOSS= [11.892999649047852, 0.0] D_LOSS= [0.16279055178165436, 0.5]\n",
      "EPOCH= 1 BATCH= 279 G_LOSS= [12.057389259338379, 0.0] D_LOSS= [0.16282173991203308, 0.5]\n",
      "EPOCH= 1 BATCH= 280 G_LOSS= [12.109857559204102, 0.0] D_LOSS= [0.16270092129707336, 0.5]\n",
      "EPOCH= 1 BATCH= 281 G_LOSS= [11.973508834838867, 0.0] D_LOSS= [0.16274391114711761, 0.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH= 1 BATCH= 282 G_LOSS= [11.998025894165039, 0.0] D_LOSS= [0.16274429857730865, 0.5]\n",
      "EPOCH= 1 BATCH= 283 G_LOSS= [12.181488037109375, 0.0] D_LOSS= [0.16273978352546692, 0.5]\n",
      "EPOCH= 1 BATCH= 284 G_LOSS= [12.028395652770996, 0.0] D_LOSS= [0.16283246874809265, 0.5]\n",
      "EPOCH= 1 BATCH= 285 G_LOSS= [12.047834396362305, 0.0] D_LOSS= [0.1627422720193863, 0.5]\n",
      "EPOCH= 1 BATCH= 286 G_LOSS= [12.093996047973633, 0.0] D_LOSS= [0.16269715130329132, 0.5]\n",
      "EPOCH= 1 BATCH= 287 G_LOSS= [12.071483612060547, 0.0] D_LOSS= [0.16269448399543762, 0.5]\n",
      "EPOCH= 1 BATCH= 288 G_LOSS= [12.125840187072754, 0.0] D_LOSS= [0.1627337634563446, 0.5]\n",
      "EPOCH= 1 BATCH= 289 G_LOSS= [12.040766716003418, 0.0] D_LOSS= [0.16273993253707886, 0.5]\n",
      "EPOCH= 1 BATCH= 290 G_LOSS= [12.04292106628418, 0.0] D_LOSS= [0.1627175509929657, 0.5]\n",
      "EPOCH= 1 BATCH= 291 G_LOSS= [12.11871337890625, 0.0] D_LOSS= [0.162693589925766, 0.5]\n",
      "EPOCH= 1 BATCH= 292 G_LOSS= [12.044931411743164, 0.0] D_LOSS= [0.16272065043449402, 0.5]\n",
      "EPOCH= 1 BATCH= 293 G_LOSS= [12.039973258972168, 0.0] D_LOSS= [0.16268795728683472, 0.5]\n",
      "EPOCH= 1 BATCH= 294 G_LOSS= [12.183198928833008, 0.0] D_LOSS= [0.16270408034324646, 0.5]\n",
      "EPOCH= 1 BATCH= 295 G_LOSS= [12.135473251342773, 0.0] D_LOSS= [0.1627063751220703, 0.5]\n",
      "EPOCH= 1 BATCH= 296 G_LOSS= [12.018033981323242, 0.0] D_LOSS= [0.1626998782157898, 0.5]\n",
      "EPOCH= 1 BATCH= 297 G_LOSS= [12.171283721923828, 0.0] D_LOSS= [0.162815660238266, 0.5]\n",
      "EPOCH= 1 BATCH= 298 G_LOSS= [12.207416534423828, 0.0] D_LOSS= [0.1627371460199356, 0.5]\n",
      "EPOCH= 1 BATCH= 299 G_LOSS= [12.105457305908203, 0.0] D_LOSS= [0.1626840978860855, 0.5]\n",
      "EPOCH= 1 BATCH= 300 G_LOSS= [12.13658332824707, 0.0] D_LOSS= [0.1626887172460556, 0.5]\n",
      "EPOCH= 1 BATCH= 301 G_LOSS= [12.26246452331543, 0.0] D_LOSS= [0.16271355748176575, 0.5]\n",
      "EPOCH= 1 BATCH= 302 G_LOSS= [12.16841983795166, 0.0] D_LOSS= [0.16272154450416565, 0.5]\n",
      "EPOCH= 1 BATCH= 303 G_LOSS= [12.1298828125, 0.0] D_LOSS= [0.16265612840652466, 0.5]\n",
      "EPOCH= 1 BATCH= 304 G_LOSS= [12.34400749206543, 0.0] D_LOSS= [0.16277334094047546, 0.5]\n",
      "EPOCH= 1 BATCH= 305 G_LOSS= [12.263229370117188, 0.0] D_LOSS= [0.16277167201042175, 0.5]\n",
      "EPOCH= 1 BATCH= 306 G_LOSS= [12.122773170471191, 0.0] D_LOSS= [0.1626977026462555, 0.5]\n",
      "EPOCH= 1 BATCH= 307 G_LOSS= [12.335681915283203, 0.0] D_LOSS= [0.16287776827812195, 0.5]\n",
      "EPOCH= 1 BATCH= 308 G_LOSS= [12.41788101196289, 0.0] D_LOSS= [0.16271233558654785, 0.5]\n",
      "EPOCH= 1 BATCH= 309 G_LOSS= [12.125182151794434, 0.0] D_LOSS= [0.16286371648311615, 0.5]\n",
      "EPOCH= 1 BATCH= 310 G_LOSS= [12.261133193969727, 0.0] D_LOSS= [0.1628943681716919, 0.5]\n",
      "EPOCH= 1 BATCH= 311 G_LOSS= [12.465560913085938, 0.0] D_LOSS= [0.16268716752529144, 0.5]\n",
      "EPOCH= 1 BATCH= 312 G_LOSS= [12.192503929138184, 0.0] D_LOSS= [0.16297510266304016, 0.5]\n",
      "EPOCH= 1 BATCH= 313 G_LOSS= [12.223231315612793, 0.0] D_LOSS= [0.16276945173740387, 0.5]\n",
      "EPOCH= 1 BATCH= 314 G_LOSS= [12.420564651489258, 0.0] D_LOSS= [0.162761390209198, 0.5]\n",
      "EPOCH= 1 BATCH= 315 G_LOSS= [12.300312042236328, 0.0] D_LOSS= [0.16282403469085693, 0.5]\n",
      "EPOCH= 1 BATCH= 316 G_LOSS= [12.199280738830566, 0.0] D_LOSS= [0.16265353560447693, 0.5]\n",
      "EPOCH= 1 BATCH= 317 G_LOSS= [12.318368911743164, 0.0] D_LOSS= [0.16274207830429077, 0.5]\n",
      "EPOCH= 1 BATCH= 318 G_LOSS= [12.134486198425293, 0.0] D_LOSS= [0.16267555952072144, 0.5]\n",
      "EPOCH= 1 BATCH= 319 G_LOSS= [12.03146743774414, 0.0] D_LOSS= [0.1627500057220459, 0.5]\n",
      "EPOCH= 1 BATCH= 320 G_LOSS= [11.973936080932617, 0.0] D_LOSS= [0.16270752251148224, 0.5]\n",
      "EPOCH= 1 BATCH= 321 G_LOSS= [12.160649299621582, 0.0] D_LOSS= [0.16275854408740997, 0.5]\n",
      "EPOCH= 1 BATCH= 322 G_LOSS= [12.155287742614746, 0.0] D_LOSS= [0.16274970769882202, 0.5]\n",
      "EPOCH= 1 BATCH= 323 G_LOSS= [12.054228782653809, 0.0] D_LOSS= [0.16271978616714478, 0.5]\n",
      "EPOCH= 1 BATCH= 324 G_LOSS= [12.24516487121582, 0.0] D_LOSS= [0.16280128061771393, 0.5]\n",
      "EPOCH= 1 BATCH= 325 G_LOSS= [12.24699592590332, 0.0] D_LOSS= [0.16270491480827332, 0.5]\n",
      "EPOCH= 1 BATCH= 326 G_LOSS= [12.19355583190918, 0.0] D_LOSS= [0.16272136569023132, 0.5]\n",
      "EPOCH= 1 BATCH= 327 G_LOSS= [12.235304832458496, 0.0] D_LOSS= [0.16271302103996277, 0.5]\n",
      "EPOCH= 1 BATCH= 328 G_LOSS= [12.287236213684082, 0.0] D_LOSS= [0.16268008947372437, 0.5]\n",
      "EPOCH= 1 BATCH= 329 G_LOSS= [12.285246849060059, 0.0] D_LOSS= [0.16267400979995728, 0.5]\n",
      "EPOCH= 1 BATCH= 330 G_LOSS= [12.229490280151367, 0.0] D_LOSS= [0.1626708060503006, 0.5]\n",
      "EPOCH= 1 BATCH= 331 G_LOSS= [12.263733863830566, 0.0] D_LOSS= [0.16272082924842834, 0.5]\n",
      "EPOCH= 1 BATCH= 332 G_LOSS= [12.292204856872559, 0.0] D_LOSS= [0.16269375383853912, 0.5]\n",
      "EPOCH= 1 BATCH= 333 G_LOSS= [12.237340927124023, 0.0] D_LOSS= [0.1627269983291626, 0.5]\n",
      "EPOCH= 1 BATCH= 334 G_LOSS= [12.281248092651367, 0.0] D_LOSS= [0.1626962423324585, 0.5]\n",
      "EPOCH= 1 BATCH= 335 G_LOSS= [12.310622215270996, 0.0] D_LOSS= [0.1627213954925537, 0.5]\n",
      "EPOCH= 1 BATCH= 336 G_LOSS= [12.27665901184082, 0.0] D_LOSS= [0.16266676783561707, 0.5]\n",
      "EPOCH= 1 BATCH= 337 G_LOSS= [12.219610214233398, 0.0] D_LOSS= [0.16269099712371826, 0.5]\n",
      "EPOCH= 1 BATCH= 338 G_LOSS= [12.306340217590332, 0.0] D_LOSS= [0.16270673274993896, 0.5]\n",
      "EPOCH= 1 BATCH= 339 G_LOSS= [12.304584503173828, 0.0] D_LOSS= [0.1626795530319214, 0.5]\n",
      "EPOCH= 1 BATCH= 340 G_LOSS= [12.160455703735352, 0.0] D_LOSS= [0.16271398961544037, 0.5]\n",
      "EPOCH= 1 BATCH= 341 G_LOSS= [12.189778327941895, 0.0] D_LOSS= [0.16276904940605164, 0.5]\n",
      "EPOCH= 1 BATCH= 342 G_LOSS= [12.292160034179688, 0.0] D_LOSS= [0.16272103786468506, 0.5]\n",
      "EPOCH= 1 BATCH= 343 G_LOSS= [12.228266716003418, 0.0] D_LOSS= [0.16269773244857788, 0.5]\n",
      "EPOCH= 1 BATCH= 344 G_LOSS= [12.139476776123047, 0.0] D_LOSS= [0.1626589000225067, 0.5]\n",
      "EPOCH= 1 BATCH= 345 G_LOSS= [12.319799423217773, 0.0] D_LOSS= [0.1627529114484787, 0.5]\n",
      "EPOCH= 1 BATCH= 346 G_LOSS= [12.207139015197754, 0.0] D_LOSS= [0.1627539098262787, 0.5]\n",
      "EPOCH= 1 BATCH= 347 G_LOSS= [12.18736457824707, 0.0] D_LOSS= [0.1627000868320465, 0.5]\n",
      "EPOCH= 1 BATCH= 348 G_LOSS= [12.300431251525879, 0.0] D_LOSS= [0.16270102560520172, 0.5]\n",
      "EPOCH= 1 BATCH= 349 G_LOSS= [12.300063133239746, 0.0] D_LOSS= [0.1626918613910675, 0.5]\n",
      "EPOCH= 1 BATCH= 350 G_LOSS= [12.19231128692627, 0.0] D_LOSS= [0.162686288356781, 0.5]\n",
      "EPOCH= 1 BATCH= 351 G_LOSS= [12.330103874206543, 0.0] D_LOSS= [0.16273146867752075, 0.5]\n",
      "EPOCH= 1 BATCH= 352 G_LOSS= [12.4010009765625, 0.0] D_LOSS= [0.16267246007919312, 0.5]\n",
      "EPOCH= 1 BATCH= 353 G_LOSS= [12.217743873596191, 0.0] D_LOSS= [0.16277940571308136, 0.5]\n",
      "EPOCH= 1 BATCH= 354 G_LOSS= [12.323002815246582, 0.0] D_LOSS= [0.1627684235572815, 0.5]\n",
      "EPOCH= 1 BATCH= 355 G_LOSS= [12.447066307067871, 0.0] D_LOSS= [0.16265004873275757, 0.5]\n",
      "EPOCH= 1 BATCH= 356 G_LOSS= [12.26960563659668, 0.0] D_LOSS= [0.1628032922744751, 0.5]\n",
      "EPOCH= 1 BATCH= 357 G_LOSS= [12.283349990844727, 0.0] D_LOSS= [0.16269701719284058, 0.5]\n",
      "EPOCH= 1 BATCH= 358 G_LOSS= [12.463337898254395, 0.0] D_LOSS= [0.16271251440048218, 0.5]\n",
      "EPOCH= 1 BATCH= 359 G_LOSS= [12.40664291381836, 0.0] D_LOSS= [0.1627385914325714, 0.5]\n",
      "EPOCH= 1 BATCH= 360 G_LOSS= [12.30122184753418, 0.0] D_LOSS= [0.16266627609729767, 0.5]\n",
      "EPOCH= 1 BATCH= 361 G_LOSS= [12.451643943786621, 0.0] D_LOSS= [0.16274693608283997, 0.5]\n",
      "EPOCH= 1 BATCH= 362 G_LOSS= [12.498178482055664, 0.0] D_LOSS= [0.16267487406730652, 0.5]\n",
      "EPOCH= 1 BATCH= 363 G_LOSS= [12.43126392364502, 0.0] D_LOSS= [0.16265201568603516, 0.5]\n",
      "EPOCH= 1 BATCH= 364 G_LOSS= [12.47043228149414, 0.0] D_LOSS= [0.16268163919448853, 0.5]\n",
      "EPOCH= 1 BATCH= 365 G_LOSS= [12.588891983032227, 0.0] D_LOSS= [0.16268205642700195, 0.5]\n",
      "EPOCH= 1 BATCH= 366 G_LOSS= [12.502243995666504, 0.0] D_LOSS= [0.16273686289787292, 0.5]\n",
      "EPOCH= 1 BATCH= 367 G_LOSS= [12.451140403747559, 0.0] D_LOSS= [0.16262151300907135, 0.5]\n",
      "EPOCH= 1 BATCH= 368 G_LOSS= [12.598755836486816, 0.0] D_LOSS= [0.16271346807479858, 0.5]\n",
      "EPOCH= 1 BATCH= 369 G_LOSS= [12.548079490661621, 0.0] D_LOSS= [0.16272962093353271, 0.5]\n",
      "EPOCH= 1 BATCH= 370 G_LOSS= [12.436823844909668, 0.0] D_LOSS= [0.1627124845981598, 0.5]\n",
      "EPOCH= 1 BATCH= 371 G_LOSS= [12.559117317199707, 0.0] D_LOSS= [0.16272813081741333, 0.5]\n",
      "EPOCH= 1 BATCH= 372 G_LOSS= [12.535889625549316, 0.0] D_LOSS= [0.1627095341682434, 0.5]\n",
      "EPOCH= 1 BATCH= 373 G_LOSS= [12.465936660766602, 0.0] D_LOSS= [0.1626821756362915, 0.5]\n",
      "EPOCH= 1 BATCH= 374 G_LOSS= [12.514007568359375, 0.0] D_LOSS= [0.16270232200622559, 0.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH= 1 BATCH= 375 G_LOSS= [12.545180320739746, 0.0] D_LOSS= [0.1626296490430832, 0.5]\n",
      "EPOCH= 1 BATCH= 376 G_LOSS= [12.447127342224121, 0.0] D_LOSS= [0.16269178688526154, 0.5]\n",
      "EPOCH= 1 BATCH= 377 G_LOSS= [12.499171257019043, 0.0] D_LOSS= [0.16269779205322266, 0.5]\n",
      "EPOCH= 1 BATCH= 378 G_LOSS= [12.534160614013672, 0.0] D_LOSS= [0.16264517605304718, 0.5]\n",
      "EPOCH= 1 BATCH= 379 G_LOSS= [12.509571075439453, 0.0] D_LOSS= [0.162690207362175, 0.5]\n",
      "EPOCH= 1 BATCH= 380 G_LOSS= [12.463714599609375, 0.0] D_LOSS= [0.1627461016178131, 0.5]\n",
      "EPOCH= 1 BATCH= 381 G_LOSS= [12.481019020080566, 0.0] D_LOSS= [0.16267022490501404, 0.5]\n",
      "EPOCH= 1 BATCH= 382 G_LOSS= [12.50102710723877, 0.0] D_LOSS= [0.1626662164926529, 0.5]\n",
      "EPOCH= 1 BATCH= 383 G_LOSS= [12.455687522888184, 0.0] D_LOSS= [0.16267403960227966, 0.5]\n",
      "EPOCH= 1 BATCH= 384 G_LOSS= [12.471343994140625, 0.0] D_LOSS= [0.16264647245407104, 0.5]\n",
      "EPOCH= 1 BATCH= 385 G_LOSS= [12.488780975341797, 0.0] D_LOSS= [0.16264301538467407, 0.5]\n",
      "EPOCH= 1 BATCH= 386 G_LOSS= [12.48867416381836, 0.0] D_LOSS= [0.16268880665302277, 0.5]\n",
      "EPOCH= 1 BATCH= 387 G_LOSS= [12.449217796325684, 0.0] D_LOSS= [0.16267076134681702, 0.5]\n",
      "EPOCH= 1 BATCH= 388 G_LOSS= [12.541834831237793, 0.0] D_LOSS= [0.16268280148506165, 0.5]\n",
      "EPOCH= 1 BATCH= 389 G_LOSS= [12.518158912658691, 0.0] D_LOSS= [0.16270068287849426, 0.5]\n",
      "EPOCH= 1 BATCH= 390 G_LOSS= [12.476804733276367, 0.0] D_LOSS= [0.16266679763793945, 0.5]\n",
      "EPOCH= 1 BATCH= 391 G_LOSS= [12.617386817932129, 0.0] D_LOSS= [0.16271556913852692, 0.5]\n",
      "EPOCH= 1 BATCH= 392 G_LOSS= [12.54693603515625, 0.0] D_LOSS= [0.16272743046283722, 0.5]\n",
      "EPOCH= 1 BATCH= 393 G_LOSS= [12.559199333190918, 0.0] D_LOSS= [0.16263176500797272, 0.5]\n",
      "EPOCH= 1 BATCH= 394 G_LOSS= [12.657044410705566, 0.0] D_LOSS= [0.16267625987529755, 0.5]\n",
      "EPOCH= 1 BATCH= 395 G_LOSS= [12.581894874572754, 0.0] D_LOSS= [0.16266459226608276, 0.5]\n",
      "EPOCH= 1 BATCH= 396 G_LOSS= [12.559818267822266, 0.0] D_LOSS= [0.16266100108623505, 0.5]\n",
      "EPOCH= 1 BATCH= 397 G_LOSS= [12.64096450805664, 0.0] D_LOSS= [0.16265027225017548, 0.5]\n",
      "EPOCH= 1 BATCH= 398 G_LOSS= [12.598494529724121, 0.0] D_LOSS= [0.16268156468868256, 0.5]\n",
      "EPOCH= 1 BATCH= 399 G_LOSS= [12.55599594116211, 0.0] D_LOSS= [0.16266480088233948, 0.5]\n",
      "EPOCH= 1 BATCH= 400 G_LOSS= [12.658783912658691, 0.0] D_LOSS= [0.16266915202140808, 0.5]\n",
      "EPOCH= 1 BATCH= 401 G_LOSS= [12.639982223510742, 0.0] D_LOSS= [0.16266442835330963, 0.5]\n",
      "EPOCH= 1 BATCH= 402 G_LOSS= [12.557219505310059, 0.0] D_LOSS= [0.1626451015472412, 0.5]\n",
      "EPOCH= 1 BATCH= 403 G_LOSS= [12.662643432617188, 0.0] D_LOSS= [0.16268590092658997, 0.5]\n",
      "EPOCH= 1 BATCH= 404 G_LOSS= [12.665889739990234, 0.0] D_LOSS= [0.16266049444675446, 0.5]\n",
      "EPOCH= 1 BATCH= 405 G_LOSS= [12.569622039794922, 0.0] D_LOSS= [0.16264823079109192, 0.5]\n",
      "EPOCH= 1 BATCH= 406 G_LOSS= [12.671921730041504, 0.0] D_LOSS= [0.16274169087409973, 0.5]\n",
      "EPOCH= 1 BATCH= 407 G_LOSS= [12.75535774230957, 0.0] D_LOSS= [0.16265133023262024, 0.5]\n",
      "EPOCH= 1 BATCH= 408 G_LOSS= [12.586009979248047, 0.0] D_LOSS= [0.1627120077610016, 0.5]\n",
      "EPOCH= 1 BATCH= 409 G_LOSS= [12.601163864135742, 0.0] D_LOSS= [0.16266873478889465, 0.5]\n",
      "EPOCH= 1 BATCH= 410 G_LOSS= [12.775542259216309, 0.0] D_LOSS= [0.16269616782665253, 0.5]\n",
      "EPOCH= 1 BATCH= 411 G_LOSS= [12.723307609558105, 0.0] D_LOSS= [0.16273823380470276, 0.5]\n",
      "EPOCH= 1 BATCH= 412 G_LOSS= [12.6248779296875, 0.0] D_LOSS= [0.16265836358070374, 0.5]\n",
      "EPOCH= 1 BATCH= 413 G_LOSS= [12.82651138305664, 0.0] D_LOSS= [0.16273042559623718, 0.5]\n",
      "EPOCH= 1 BATCH= 414 G_LOSS= [12.762772560119629, 0.0] D_LOSS= [0.16271944344043732, 0.5]\n",
      "EPOCH= 1 BATCH= 415 G_LOSS= [12.572382926940918, 0.0] D_LOSS= [0.16265933215618134, 0.5]\n",
      "EPOCH= 1 BATCH= 416 G_LOSS= [12.864195823669434, 0.0] D_LOSS= [0.1628873497247696, 0.5]\n",
      "EPOCH= 1 BATCH= 417 G_LOSS= [12.812973022460938, 0.0] D_LOSS= [0.162786602973938, 0.5]\n",
      "EPOCH= 1 BATCH= 418 G_LOSS= [12.635705947875977, 0.0] D_LOSS= [0.16268518567085266, 0.5]\n",
      "EPOCH= 1 BATCH= 419 G_LOSS= [12.77540397644043, 0.0] D_LOSS= [0.1627742350101471, 0.5]\n",
      "EPOCH= 1 BATCH= 420 G_LOSS= [12.867173194885254, 0.0] D_LOSS= [0.16267745196819305, 0.5]\n",
      "EPOCH= 1 BATCH= 421 G_LOSS= [12.733643531799316, 0.0] D_LOSS= [0.16273176670074463, 0.5]\n",
      "EPOCH= 1 BATCH= 422 G_LOSS= [12.73516845703125, 0.0] D_LOSS= [0.16266512870788574, 0.5]\n",
      "EPOCH= 1 BATCH= 423 G_LOSS= [12.863511085510254, 0.0] D_LOSS= [0.1626717448234558, 0.5]\n",
      "EPOCH= 1 BATCH= 424 G_LOSS= [12.72903060913086, 0.0] D_LOSS= [0.16270947456359863, 0.5]\n",
      "EPOCH= 1 BATCH= 425 G_LOSS= [12.759258270263672, 0.0] D_LOSS= [0.16269899904727936, 0.5]\n",
      "EPOCH= 1 BATCH= 426 G_LOSS= [12.878376007080078, 0.0] D_LOSS= [0.16265803575515747, 0.5]\n",
      "EPOCH= 1 BATCH= 427 G_LOSS= [12.697251319885254, 0.0] D_LOSS= [0.1627885103225708, 0.5]\n",
      "EPOCH= 1 BATCH= 428 G_LOSS= [12.76594352722168, 0.0] D_LOSS= [0.16270703077316284, 0.5]\n",
      "EPOCH= 1 BATCH= 429 G_LOSS= [12.89272689819336, 0.0] D_LOSS= [0.16266514360904694, 0.5]\n",
      "EPOCH= 1 BATCH= 430 G_LOSS= [12.659778594970703, 0.0] D_LOSS= [0.16279573738574982, 0.5]\n",
      "EPOCH= 1 BATCH= 431 G_LOSS= [12.765331268310547, 0.0] D_LOSS= [0.16280415654182434, 0.5]\n",
      "EPOCH= 1 BATCH= 432 G_LOSS= [12.950135231018066, 0.0] D_LOSS= [0.16268539428710938, 0.5]\n",
      "EPOCH= 1 BATCH= 433 G_LOSS= [12.707108497619629, 0.0] D_LOSS= [0.16283923387527466, 0.5]\n",
      "EPOCH= 1 BATCH= 434 G_LOSS= [12.782179832458496, 0.0] D_LOSS= [0.16277077794075012, 0.5]\n",
      "EPOCH= 1 BATCH= 435 G_LOSS= [12.969010353088379, 0.0] D_LOSS= [0.16265329718589783, 0.5]\n",
      "EPOCH= 1 BATCH= 436 G_LOSS= [12.823988914489746, 0.0] D_LOSS= [0.16279523074626923, 0.5]\n",
      "EPOCH= 1 BATCH= 437 G_LOSS= [12.798537254333496, 0.0] D_LOSS= [0.16266977787017822, 0.5]\n",
      "EPOCH= 1 BATCH= 438 G_LOSS= [13.004075050354004, 0.0] D_LOSS= [0.1627054661512375, 0.5]\n",
      "EPOCH= 1 BATCH= 439 G_LOSS= [12.842218399047852, 0.0] D_LOSS= [0.1627705693244934, 0.5]\n",
      "EPOCH= 1 BATCH= 440 G_LOSS= [12.802974700927734, 0.0] D_LOSS= [0.162677600979805, 0.5]\n",
      "EPOCH= 1 BATCH= 441 G_LOSS= [12.961121559143066, 0.0] D_LOSS= [0.16267678141593933, 0.5]\n",
      "EPOCH= 1 BATCH= 442 G_LOSS= [12.85861587524414, 0.0] D_LOSS= [0.16272249817848206, 0.5]\n",
      "EPOCH= 1 BATCH= 443 G_LOSS= [12.857852935791016, 0.0] D_LOSS= [0.16266292333602905, 0.5]\n",
      "EPOCH= 1 BATCH= 444 G_LOSS= [12.861861228942871, 0.0] D_LOSS= [0.16264605522155762, 0.5]\n",
      "EPOCH= 1 BATCH= 445 G_LOSS= [12.907759666442871, 0.0] D_LOSS= [0.16265122592449188, 0.5]\n",
      "EPOCH= 1 BATCH= 446 G_LOSS= [12.884825706481934, 0.0] D_LOSS= [0.16265588998794556, 0.5]\n",
      "EPOCH= 1 BATCH= 447 G_LOSS= [12.84857177734375, 0.0] D_LOSS= [0.1626288890838623, 0.5]\n",
      "EPOCH= 1 BATCH= 448 G_LOSS= [12.833802223205566, 0.0] D_LOSS= [0.1626405268907547, 0.5]\n",
      "EPOCH= 1 BATCH= 449 G_LOSS= [12.87005615234375, 0.0] D_LOSS= [0.16268683969974518, 0.5]\n",
      "EPOCH= 1 BATCH= 450 G_LOSS= [12.854247093200684, 0.0] D_LOSS= [0.16268432140350342, 0.5]\n",
      "EPOCH= 1 BATCH= 451 G_LOSS= [12.745965957641602, 0.0] D_LOSS= [0.16264094412326813, 0.5]\n",
      "EPOCH= 1 BATCH= 452 G_LOSS= [12.794480323791504, 0.0] D_LOSS= [0.16265949606895447, 0.5]\n",
      "EPOCH= 1 BATCH= 453 G_LOSS= [12.822602272033691, 0.0] D_LOSS= [0.16262760758399963, 0.5]\n",
      "EPOCH= 1 BATCH= 454 G_LOSS= [12.708037376403809, 0.0] D_LOSS= [0.16267749667167664, 0.5]\n",
      "EPOCH= 1 BATCH= 455 G_LOSS= [12.757096290588379, 0.0] D_LOSS= [0.16265293955802917, 0.5]\n",
      "EPOCH= 1 BATCH= 456 G_LOSS= [12.687564849853516, 0.0] D_LOSS= [0.16271217167377472, 0.5]\n",
      "EPOCH= 1 BATCH= 457 G_LOSS= [12.75651741027832, 0.0] D_LOSS= [0.16266800463199615, 0.5]\n",
      "EPOCH= 1 BATCH= 458 G_LOSS= [12.730555534362793, 0.0] D_LOSS= [0.1626390665769577, 0.5]\n",
      "EPOCH= 1 BATCH= 459 G_LOSS= [12.672348976135254, 0.0] D_LOSS= [0.16261693835258484, 0.5]\n",
      "EPOCH= 1 BATCH= 460 G_LOSS= [12.736832618713379, 0.0] D_LOSS= [0.16264085471630096, 0.5]\n",
      "EPOCH= 1 BATCH= 461 G_LOSS= [12.700105667114258, 0.0] D_LOSS= [0.16265296936035156, 0.5]\n",
      "EPOCH= 1 BATCH= 462 G_LOSS= [12.692974090576172, 0.0] D_LOSS= [0.16267168521881104, 0.5]\n",
      "EPOCH= 1 BATCH= 463 G_LOSS= [12.758759498596191, 0.0] D_LOSS= [0.16264566779136658, 0.5]\n",
      "EPOCH= 1 BATCH= 464 G_LOSS= [12.68067455291748, 0.0] D_LOSS= [0.16265922784805298, 0.5]\n",
      "EPOCH= 1 BATCH= 465 G_LOSS= [12.663724899291992, 0.0] D_LOSS= [0.16262361407279968, 0.5]\n",
      "EPOCH= 1 BATCH= 466 G_LOSS= [12.819438934326172, 0.0] D_LOSS= [0.16265493631362915, 0.5]\n",
      "EPOCH= 1 BATCH= 467 G_LOSS= [12.610136032104492, 0.0] D_LOSS= [0.16277655959129333, 0.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH= 1 BATCH= 468 G_LOSS= [12.72901439666748, 0.0] D_LOSS= [0.16272641718387604, 0.5]\n",
      "EPOCH= 2 BATCH= 1 G_LOSS= [12.822132110595703, 0.0] D_LOSS= [0.16264432668685913, 0.5]\n",
      "EPOCH= 2 BATCH= 2 G_LOSS= [12.721970558166504, 0.0] D_LOSS= [0.16269099712371826, 0.5]\n",
      "EPOCH= 2 BATCH= 3 G_LOSS= [12.743881225585938, 0.0] D_LOSS= [0.16267021000385284, 0.5]\n",
      "EPOCH= 2 BATCH= 4 G_LOSS= [12.928481101989746, 0.0] D_LOSS= [0.16267414391040802, 0.5]\n",
      "EPOCH= 2 BATCH= 5 G_LOSS= [12.77981185913086, 0.0] D_LOSS= [0.16275262832641602, 0.5]\n",
      "EPOCH= 2 BATCH= 6 G_LOSS= [12.833939552307129, 0.0] D_LOSS= [0.1626516729593277, 0.5]\n",
      "EPOCH= 2 BATCH= 7 G_LOSS= [13.013225555419922, 0.0] D_LOSS= [0.16265982389450073, 0.5]\n",
      "EPOCH= 2 BATCH= 8 G_LOSS= [12.778322219848633, 0.0] D_LOSS= [0.16280019283294678, 0.5]\n",
      "EPOCH= 2 BATCH= 9 G_LOSS= [12.981172561645508, 0.0] D_LOSS= [0.16283057630062103, 0.5]\n",
      "EPOCH= 2 BATCH= 10 G_LOSS= [13.021653175354004, 0.0] D_LOSS= [0.16264313459396362, 0.5]\n",
      "EPOCH= 2 BATCH= 11 G_LOSS= [12.832767486572266, 0.0] D_LOSS= [0.16269144415855408, 0.5]\n",
      "EPOCH= 2 BATCH= 12 G_LOSS= [12.991443634033203, 0.0] D_LOSS= [0.16276845335960388, 0.5]\n",
      "EPOCH= 2 BATCH= 13 G_LOSS= [13.017518043518066, 0.0] D_LOSS= [0.16267314553260803, 0.5]\n",
      "EPOCH= 2 BATCH= 14 G_LOSS= [12.833479881286621, 0.0] D_LOSS= [0.16272497177124023, 0.5]\n",
      "EPOCH= 2 BATCH= 15 G_LOSS= [12.964141845703125, 0.0] D_LOSS= [0.1627519726753235, 0.5]\n",
      "EPOCH= 2 BATCH= 16 G_LOSS= [13.01254653930664, 0.0] D_LOSS= [0.16269218921661377, 0.5]\n",
      "EPOCH= 2 BATCH= 17 G_LOSS= [12.848343849182129, 0.0] D_LOSS= [0.16268673539161682, 0.5]\n",
      "EPOCH= 2 BATCH= 18 G_LOSS= [12.921428680419922, 0.0] D_LOSS= [0.1626998335123062, 0.5]\n",
      "EPOCH= 2 BATCH= 19 G_LOSS= [12.924938201904297, 0.0] D_LOSS= [0.16263370215892792, 0.5]\n",
      "EPOCH= 2 BATCH= 20 G_LOSS= [12.869665145874023, 0.0] D_LOSS= [0.16265863180160522, 0.5]\n",
      "EPOCH= 2 BATCH= 21 G_LOSS= [12.877252578735352, 0.0] D_LOSS= [0.1626838743686676, 0.5]\n",
      "EPOCH= 2 BATCH= 22 G_LOSS= [12.858745574951172, 0.0] D_LOSS= [0.16265854239463806, 0.5]\n",
      "EPOCH= 2 BATCH= 23 G_LOSS= [12.803936004638672, 0.0] D_LOSS= [0.1626741886138916, 0.5]\n",
      "EPOCH= 2 BATCH= 24 G_LOSS= [12.895442962646484, 0.0] D_LOSS= [0.1626514345407486, 0.5]\n",
      "EPOCH= 2 BATCH= 25 G_LOSS= [12.76970386505127, 0.0] D_LOSS= [0.1626940369606018, 0.5]\n",
      "EPOCH= 2 BATCH= 26 G_LOSS= [12.72169017791748, 0.0] D_LOSS= [0.16264978051185608, 0.5]\n",
      "EPOCH= 2 BATCH= 27 G_LOSS= [12.838314056396484, 0.0] D_LOSS= [0.16265764832496643, 0.5]\n",
      "EPOCH= 2 BATCH= 28 G_LOSS= [12.76402473449707, 0.0] D_LOSS= [0.16267013549804688, 0.5]\n",
      "EPOCH= 2 BATCH= 29 G_LOSS= [12.743345260620117, 0.0] D_LOSS= [0.16266262531280518, 0.5]\n",
      "EPOCH= 2 BATCH= 30 G_LOSS= [12.768834114074707, 0.0] D_LOSS= [0.1626085638999939, 0.5]\n",
      "EPOCH= 2 BATCH= 31 G_LOSS= [12.69023323059082, 0.0] D_LOSS= [0.16263653337955475, 0.5]\n",
      "EPOCH= 2 BATCH= 32 G_LOSS= [12.768693923950195, 0.0] D_LOSS= [0.1626720279455185, 0.5]\n",
      "EPOCH= 2 BATCH= 33 G_LOSS= [12.75922679901123, 0.0] D_LOSS= [0.16264167428016663, 0.5]\n",
      "EPOCH= 2 BATCH= 34 G_LOSS= [12.673264503479004, 0.0] D_LOSS= [0.1626465618610382, 0.5]\n",
      "EPOCH= 2 BATCH= 35 G_LOSS= [12.789962768554688, 0.0] D_LOSS= [0.16267046332359314, 0.5]\n",
      "EPOCH= 2 BATCH= 36 G_LOSS= [12.758403778076172, 0.0] D_LOSS= [0.16267386078834534, 0.5]\n",
      "EPOCH= 2 BATCH= 37 G_LOSS= [12.679102897644043, 0.0] D_LOSS= [0.1626317948102951, 0.5]\n",
      "EPOCH= 2 BATCH= 38 G_LOSS= [12.794459342956543, 0.0] D_LOSS= [0.16267156600952148, 0.5]\n",
      "EPOCH= 2 BATCH= 39 G_LOSS= [12.781325340270996, 0.0] D_LOSS= [0.16265982389450073, 0.5]\n",
      "EPOCH= 2 BATCH= 40 G_LOSS= [12.671784400939941, 0.0] D_LOSS= [0.16267290711402893, 0.5]\n",
      "EPOCH= 2 BATCH= 41 G_LOSS= [12.80169677734375, 0.0] D_LOSS= [0.16267748177051544, 0.5]\n",
      "EPOCH= 2 BATCH= 42 G_LOSS= [12.792939186096191, 0.0] D_LOSS= [0.16266517341136932, 0.5]\n",
      "EPOCH= 2 BATCH= 43 G_LOSS= [12.659826278686523, 0.0] D_LOSS= [0.16266249120235443, 0.5]\n",
      "EPOCH= 2 BATCH= 44 G_LOSS= [12.848087310791016, 0.0] D_LOSS= [0.16272278130054474, 0.5]\n",
      "EPOCH= 2 BATCH= 45 G_LOSS= [12.769026756286621, 0.0] D_LOSS= [0.16268755495548248, 0.5]\n",
      "EPOCH= 2 BATCH= 46 G_LOSS= [12.728246688842773, 0.0] D_LOSS= [0.1626114547252655, 0.5]\n",
      "EPOCH= 2 BATCH= 47 G_LOSS= [12.87343978881836, 0.0] D_LOSS= [0.1626536250114441, 0.5]\n",
      "EPOCH= 2 BATCH= 48 G_LOSS= [12.847062110900879, 0.0] D_LOSS= [0.16267219185829163, 0.5]\n",
      "EPOCH= 2 BATCH= 49 G_LOSS= [12.693419456481934, 0.0] D_LOSS= [0.16264070570468903, 0.5]\n",
      "EPOCH= 2 BATCH= 50 G_LOSS= [12.953472137451172, 0.0] D_LOSS= [0.16279403865337372, 0.5]\n",
      "EPOCH= 2 BATCH= 51 G_LOSS= [12.812641143798828, 0.0] D_LOSS= [0.1627800166606903, 0.5]\n",
      "EPOCH= 2 BATCH= 52 G_LOSS= [12.794205665588379, 0.0] D_LOSS= [0.16263318061828613, 0.5]\n",
      "EPOCH= 2 BATCH= 53 G_LOSS= [12.980545043945312, 0.0] D_LOSS= [0.1626410186290741, 0.5]\n",
      "EPOCH= 2 BATCH= 54 G_LOSS= [12.863494873046875, 0.0] D_LOSS= [0.16271179914474487, 0.5]\n",
      "EPOCH= 2 BATCH= 55 G_LOSS= [12.876112937927246, 0.0] D_LOSS= [0.16264072060585022, 0.5]\n",
      "EPOCH= 2 BATCH= 56 G_LOSS= [12.995550155639648, 0.0] D_LOSS= [0.16262954473495483, 0.5]\n",
      "EPOCH= 2 BATCH= 57 G_LOSS= [12.964417457580566, 0.0] D_LOSS= [0.16267409920692444, 0.5]\n",
      "EPOCH= 2 BATCH= 58 G_LOSS= [13.039566993713379, 0.0] D_LOSS= [0.16262534260749817, 0.5]\n",
      "EPOCH= 2 BATCH= 59 G_LOSS= [13.029861450195312, 0.0] D_LOSS= [0.16263359785079956, 0.5]\n",
      "EPOCH= 2 BATCH= 60 G_LOSS= [13.049127578735352, 0.0] D_LOSS= [0.16261310875415802, 0.5]\n",
      "EPOCH= 2 BATCH= 61 G_LOSS= [13.064006805419922, 0.0] D_LOSS= [0.16262809932231903, 0.5]\n",
      "EPOCH= 2 BATCH= 62 G_LOSS= [13.115403175354004, 0.0] D_LOSS= [0.1626189798116684, 0.5]\n",
      "EPOCH= 2 BATCH= 63 G_LOSS= [13.083560943603516, 0.0] D_LOSS= [0.16264623403549194, 0.5]\n",
      "EPOCH= 2 BATCH= 64 G_LOSS= [13.102874755859375, 0.0] D_LOSS= [0.16261592507362366, 0.5]\n",
      "EPOCH= 2 BATCH= 65 G_LOSS= [13.130341529846191, 0.0] D_LOSS= [0.16264502704143524, 0.5]\n",
      "EPOCH= 2 BATCH= 66 G_LOSS= [13.115550994873047, 0.0] D_LOSS= [0.16263824701309204, 0.5]\n",
      "EPOCH= 2 BATCH= 67 G_LOSS= [13.114487648010254, 0.0] D_LOSS= [0.1626410037279129, 0.5]\n",
      "EPOCH= 2 BATCH= 68 G_LOSS= [13.131738662719727, 0.0] D_LOSS= [0.16262274980545044, 0.5]\n",
      "EPOCH= 2 BATCH= 69 G_LOSS= [13.146724700927734, 0.0] D_LOSS= [0.1626071035861969, 0.5]\n",
      "EPOCH= 2 BATCH= 70 G_LOSS= [13.122024536132812, 0.0] D_LOSS= [0.16261501610279083, 0.5]\n",
      "EPOCH= 2 BATCH= 71 G_LOSS= [13.08266830444336, 0.0] D_LOSS= [0.16264715790748596, 0.5]\n",
      "EPOCH= 2 BATCH= 72 G_LOSS= [13.193694114685059, 0.0] D_LOSS= [0.1626289337873459, 0.5]\n",
      "EPOCH= 2 BATCH= 73 G_LOSS= [13.110518455505371, 0.0] D_LOSS= [0.16266606748104095, 0.5]\n",
      "EPOCH= 2 BATCH= 74 G_LOSS= [13.093400001525879, 0.0] D_LOSS= [0.1626197099685669, 0.5]\n",
      "EPOCH= 2 BATCH= 75 G_LOSS= [13.258238792419434, 0.0] D_LOSS= [0.16265270113945007, 0.5]\n",
      "EPOCH= 2 BATCH= 76 G_LOSS= [13.10129165649414, 0.0] D_LOSS= [0.16274917125701904, 0.5]\n",
      "EPOCH= 2 BATCH= 77 G_LOSS= [13.12271499633789, 0.0] D_LOSS= [0.16267701983451843, 0.5]\n",
      "EPOCH= 2 BATCH= 78 G_LOSS= [13.265776634216309, 0.0] D_LOSS= [0.16264379024505615, 0.5]\n",
      "EPOCH= 2 BATCH= 79 G_LOSS= [13.131760597229004, 0.0] D_LOSS= [0.16270625591278076, 0.5]\n",
      "EPOCH= 2 BATCH= 80 G_LOSS= [13.10940170288086, 0.0] D_LOSS= [0.16263574361801147, 0.5]\n",
      "EPOCH= 2 BATCH= 81 G_LOSS= [13.290986061096191, 0.0] D_LOSS= [0.16267997026443481, 0.5]\n",
      "EPOCH= 2 BATCH= 82 G_LOSS= [13.222232818603516, 0.0] D_LOSS= [0.1626729667186737, 0.5]\n",
      "EPOCH= 2 BATCH= 83 G_LOSS= [13.137375831604004, 0.0] D_LOSS= [0.1626317948102951, 0.5]\n",
      "EPOCH= 2 BATCH= 84 G_LOSS= [13.339534759521484, 0.0] D_LOSS= [0.16270318627357483, 0.5]\n",
      "EPOCH= 2 BATCH= 85 G_LOSS= [13.269118309020996, 0.0] D_LOSS= [0.16268131136894226, 0.5]\n",
      "EPOCH= 2 BATCH= 86 G_LOSS= [13.109222412109375, 0.0] D_LOSS= [0.16262979805469513, 0.5]\n",
      "EPOCH= 2 BATCH= 87 G_LOSS= [13.302474975585938, 0.0] D_LOSS= [0.1627286821603775, 0.5]\n",
      "EPOCH= 2 BATCH= 88 G_LOSS= [13.300251007080078, 0.0] D_LOSS= [0.1626647263765335, 0.5]\n",
      "EPOCH= 2 BATCH= 89 G_LOSS= [13.089405059814453, 0.0] D_LOSS= [0.16267234086990356, 0.5]\n",
      "EPOCH= 2 BATCH= 90 G_LOSS= [13.360503196716309, 0.0] D_LOSS= [0.16282612085342407, 0.5]\n",
      "EPOCH= 2 BATCH= 91 G_LOSS= [13.284911155700684, 0.0] D_LOSS= [0.16273754835128784, 0.5]\n",
      "EPOCH= 2 BATCH= 92 G_LOSS= [13.04372787475586, 0.0] D_LOSS= [0.16264449059963226, 0.5]\n",
      "EPOCH= 2 BATCH= 93 G_LOSS= [13.306798934936523, 0.0] D_LOSS= [0.16287864744663239, 0.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH= 2 BATCH= 94 G_LOSS= [13.382323265075684, 0.0] D_LOSS= [0.1626356840133667, 0.5]\n",
      "EPOCH= 2 BATCH= 95 G_LOSS= [13.091126441955566, 0.0] D_LOSS= [0.16278958320617676, 0.5]\n",
      "EPOCH= 2 BATCH= 96 G_LOSS= [13.314403533935547, 0.0] D_LOSS= [0.16288460791110992, 0.5]\n",
      "EPOCH= 2 BATCH= 97 G_LOSS= [13.348857879638672, 0.0] D_LOSS= [0.16263896226882935, 0.5]\n",
      "EPOCH= 2 BATCH= 98 G_LOSS= [13.14712905883789, 0.0] D_LOSS= [0.16270962357521057, 0.5]\n",
      "EPOCH= 2 BATCH= 99 G_LOSS= [13.287952423095703, 0.0] D_LOSS= [0.16273769736289978, 0.5]\n",
      "EPOCH= 2 BATCH= 100 G_LOSS= [13.392166137695312, 0.0] D_LOSS= [0.16263478994369507, 0.5]\n",
      "EPOCH= 2 BATCH= 101 G_LOSS= [13.125061988830566, 0.0] D_LOSS= [0.16277164220809937, 0.5]\n",
      "EPOCH= 2 BATCH= 102 G_LOSS= [13.321918487548828, 0.0] D_LOSS= [0.1628323793411255, 0.5]\n",
      "EPOCH= 2 BATCH= 103 G_LOSS= [13.367938995361328, 0.0] D_LOSS= [0.16264384984970093, 0.5]\n",
      "EPOCH= 2 BATCH= 104 G_LOSS= [13.114757537841797, 0.0] D_LOSS= [0.16270554065704346, 0.5]\n",
      "EPOCH= 2 BATCH= 105 G_LOSS= [13.256961822509766, 0.0] D_LOSS= [0.1627805531024933, 0.5]\n",
      "EPOCH= 2 BATCH= 106 G_LOSS= [13.313841819763184, 0.0] D_LOSS= [0.1626245677471161, 0.5]\n",
      "EPOCH= 2 BATCH= 107 G_LOSS= [13.157652854919434, 0.0] D_LOSS= [0.16267389059066772, 0.5]\n",
      "EPOCH= 2 BATCH= 108 G_LOSS= [13.174941062927246, 0.0] D_LOSS= [0.16265034675598145, 0.5]\n",
      "EPOCH= 2 BATCH= 109 G_LOSS= [13.308365821838379, 0.0] D_LOSS= [0.16264206171035767, 0.5]\n",
      "EPOCH= 2 BATCH= 110 G_LOSS= [13.230371475219727, 0.0] D_LOSS= [0.16266193985939026, 0.5]\n",
      "EPOCH= 2 BATCH= 111 G_LOSS= [13.229251861572266, 0.0] D_LOSS= [0.1626252830028534, 0.5]\n",
      "EPOCH= 2 BATCH= 112 G_LOSS= [13.237883567810059, 0.0] D_LOSS= [0.16260412335395813, 0.5]\n",
      "EPOCH= 2 BATCH= 113 G_LOSS= [13.239505767822266, 0.0] D_LOSS= [0.16263845562934875, 0.5]\n",
      "EPOCH= 2 BATCH= 114 G_LOSS= [13.258831024169922, 0.0] D_LOSS= [0.16261836886405945, 0.5]\n",
      "EPOCH= 2 BATCH= 115 G_LOSS= [13.210445404052734, 0.0] D_LOSS= [0.16261576116085052, 0.5]\n",
      "EPOCH= 2 BATCH= 116 G_LOSS= [13.253098487854004, 0.0] D_LOSS= [0.1626492142677307, 0.5]\n",
      "EPOCH= 2 BATCH= 117 G_LOSS= [13.282428741455078, 0.0] D_LOSS= [0.16262146830558777, 0.5]\n",
      "EPOCH= 2 BATCH= 118 G_LOSS= [13.22684097290039, 0.0] D_LOSS= [0.16265803575515747, 0.5]\n",
      "EPOCH= 2 BATCH= 119 G_LOSS= [13.202587127685547, 0.0] D_LOSS= [0.16262751817703247, 0.5]\n",
      "EPOCH= 2 BATCH= 120 G_LOSS= [13.329346656799316, 0.0] D_LOSS= [0.1626378297805786, 0.5]\n",
      "EPOCH= 2 BATCH= 121 G_LOSS= [13.250736236572266, 0.0] D_LOSS= [0.16266751289367676, 0.5]\n",
      "EPOCH= 2 BATCH= 122 G_LOSS= [13.14266586303711, 0.0] D_LOSS= [0.16261374950408936, 0.5]\n",
      "EPOCH= 2 BATCH= 123 G_LOSS= [13.321684837341309, 0.0] D_LOSS= [0.16271452605724335, 0.5]\n",
      "EPOCH= 2 BATCH= 124 G_LOSS= [13.206406593322754, 0.0] D_LOSS= [0.16267544031143188, 0.5]\n",
      "EPOCH= 2 BATCH= 125 G_LOSS= [13.093581199645996, 0.0] D_LOSS= [0.16264623403549194, 0.5]\n",
      "EPOCH= 2 BATCH= 126 G_LOSS= [13.286759376525879, 0.0] D_LOSS= [0.16270053386688232, 0.5]\n",
      "EPOCH= 2 BATCH= 127 G_LOSS= [13.200852394104004, 0.0] D_LOSS= [0.16269738972187042, 0.5]\n",
      "EPOCH= 2 BATCH= 128 G_LOSS= [13.105010032653809, 0.0] D_LOSS= [0.16262203454971313, 0.5]\n",
      "EPOCH= 2 BATCH= 129 G_LOSS= [13.271742820739746, 0.0] D_LOSS= [0.1626763790845871, 0.5]\n",
      "EPOCH= 2 BATCH= 130 G_LOSS= [13.152523040771484, 0.0] D_LOSS= [0.16270621120929718, 0.5]\n",
      "EPOCH= 2 BATCH= 131 G_LOSS= [13.111021995544434, 0.0] D_LOSS= [0.16261637210845947, 0.5]\n",
      "EPOCH= 2 BATCH= 132 G_LOSS= [13.203433990478516, 0.0] D_LOSS= [0.162650465965271, 0.5]\n",
      "EPOCH= 2 BATCH= 133 G_LOSS= [13.175731658935547, 0.0] D_LOSS= [0.1626465618610382, 0.5]\n",
      "EPOCH= 2 BATCH= 134 G_LOSS= [13.082996368408203, 0.0] D_LOSS= [0.16262733936309814, 0.5]\n",
      "EPOCH= 2 BATCH= 135 G_LOSS= [13.183673858642578, 0.0] D_LOSS= [0.1626756489276886, 0.5]\n",
      "EPOCH= 2 BATCH= 136 G_LOSS= [13.175464630126953, 0.0] D_LOSS= [0.1626415103673935, 0.5]\n",
      "EPOCH= 2 BATCH= 137 G_LOSS= [13.100086212158203, 0.0] D_LOSS= [0.1626415103673935, 0.5]\n",
      "EPOCH= 2 BATCH= 138 G_LOSS= [13.207584381103516, 0.0] D_LOSS= [0.16264045238494873, 0.5]\n",
      "EPOCH= 2 BATCH= 139 G_LOSS= [13.154376983642578, 0.0] D_LOSS= [0.1626705825328827, 0.5]\n",
      "EPOCH= 2 BATCH= 140 G_LOSS= [13.159438133239746, 0.0] D_LOSS= [0.16263249516487122, 0.5]\n",
      "EPOCH= 2 BATCH= 141 G_LOSS= [13.224915504455566, 0.0] D_LOSS= [0.16262726485729218, 0.5]\n",
      "EPOCH= 2 BATCH= 142 G_LOSS= [13.13510513305664, 0.0] D_LOSS= [0.16264653205871582, 0.5]\n",
      "EPOCH= 2 BATCH= 143 G_LOSS= [13.217591285705566, 0.0] D_LOSS= [0.16264626383781433, 0.5]\n",
      "EPOCH= 2 BATCH= 144 G_LOSS= [13.254990577697754, 0.0] D_LOSS= [0.16261738538742065, 0.5]\n",
      "EPOCH= 2 BATCH= 145 G_LOSS= [13.158798217773438, 0.0] D_LOSS= [0.16266530752182007, 0.5]\n",
      "EPOCH= 2 BATCH= 146 G_LOSS= [13.175675392150879, 0.0] D_LOSS= [0.16261053085327148, 0.5]\n",
      "EPOCH= 2 BATCH= 147 G_LOSS= [13.270783424377441, 0.0] D_LOSS= [0.1626274138689041, 0.5]\n",
      "EPOCH= 2 BATCH= 148 G_LOSS= [13.245182037353516, 0.0] D_LOSS= [0.1626354455947876, 0.5]\n",
      "EPOCH= 2 BATCH= 149 G_LOSS= [13.154969215393066, 0.0] D_LOSS= [0.16264787316322327, 0.5]\n",
      "EPOCH= 2 BATCH= 150 G_LOSS= [13.31674575805664, 0.0] D_LOSS= [0.1626969575881958, 0.5]\n",
      "EPOCH= 2 BATCH= 151 G_LOSS= [13.25794792175293, 0.0] D_LOSS= [0.16264677047729492, 0.5]\n",
      "EPOCH= 2 BATCH= 152 G_LOSS= [13.207271575927734, 0.0] D_LOSS= [0.1626053750514984, 0.5]\n",
      "EPOCH= 2 BATCH= 153 G_LOSS= [13.293102264404297, 0.0] D_LOSS= [0.1626419723033905, 0.5]\n",
      "EPOCH= 2 BATCH= 154 G_LOSS= [13.290531158447266, 0.0] D_LOSS= [0.1626296043395996, 0.5]\n",
      "EPOCH= 2 BATCH= 155 G_LOSS= [13.223296165466309, 0.0] D_LOSS= [0.16262680292129517, 0.5]\n",
      "EPOCH= 2 BATCH= 156 G_LOSS= [13.322983741760254, 0.0] D_LOSS= [0.16264766454696655, 0.5]\n",
      "EPOCH= 2 BATCH= 157 G_LOSS= [13.309767723083496, 0.0] D_LOSS= [0.1626235842704773, 0.5]\n",
      "EPOCH= 2 BATCH= 158 G_LOSS= [13.252483367919922, 0.0] D_LOSS= [0.16262486577033997, 0.5]\n",
      "EPOCH= 2 BATCH= 159 G_LOSS= [13.298700332641602, 0.0] D_LOSS= [0.16262900829315186, 0.5]\n",
      "EPOCH= 2 BATCH= 160 G_LOSS= [13.430632591247559, 0.0] D_LOSS= [0.16262415051460266, 0.5]\n",
      "EPOCH= 2 BATCH= 161 G_LOSS= [13.28261947631836, 0.0] D_LOSS= [0.16267666220664978, 0.5]\n",
      "EPOCH= 2 BATCH= 162 G_LOSS= [13.289017677307129, 0.0] D_LOSS= [0.16263483464717865, 0.5]\n",
      "EPOCH= 2 BATCH= 163 G_LOSS= [13.495774269104004, 0.0] D_LOSS= [0.16264477372169495, 0.5]\n",
      "EPOCH= 2 BATCH= 164 G_LOSS= [13.284847259521484, 0.0] D_LOSS= [0.16277426481246948, 0.5]\n",
      "EPOCH= 2 BATCH= 165 G_LOSS= [13.328627586364746, 0.0] D_LOSS= [0.1626751571893692, 0.5]\n",
      "EPOCH= 2 BATCH= 166 G_LOSS= [13.466175079345703, 0.0] D_LOSS= [0.16262570023536682, 0.5]\n",
      "EPOCH= 2 BATCH= 167 G_LOSS= [13.33306884765625, 0.0] D_LOSS= [0.16270099580287933, 0.5]\n",
      "EPOCH= 2 BATCH= 168 G_LOSS= [13.307982444763184, 0.0] D_LOSS= [0.16264010965824127, 0.5]\n",
      "EPOCH= 2 BATCH= 169 G_LOSS= [13.513976097106934, 0.0] D_LOSS= [0.16267049312591553, 0.5]\n",
      "EPOCH= 2 BATCH= 170 G_LOSS= [13.337261199951172, 0.0] D_LOSS= [0.16276836395263672, 0.5]\n",
      "EPOCH= 2 BATCH= 171 G_LOSS= [13.288939476013184, 0.0] D_LOSS= [0.16264191269874573, 0.5]\n",
      "EPOCH= 2 BATCH= 172 G_LOSS= [13.47018051147461, 0.0] D_LOSS= [0.16266638040542603, 0.5]\n",
      "EPOCH= 2 BATCH= 173 G_LOSS= [13.375746726989746, 0.0] D_LOSS= [0.1626732051372528, 0.5]\n",
      "EPOCH= 2 BATCH= 174 G_LOSS= [13.281296730041504, 0.0] D_LOSS= [0.16260778903961182, 0.5]\n",
      "EPOCH= 2 BATCH= 175 G_LOSS= [13.483402252197266, 0.0] D_LOSS= [0.16272248327732086, 0.5]\n",
      "EPOCH= 2 BATCH= 176 G_LOSS= [13.346050262451172, 0.0] D_LOSS= [0.1627494841814041, 0.5]\n",
      "EPOCH= 2 BATCH= 177 G_LOSS= [13.302659034729004, 0.0] D_LOSS= [0.16262105107307434, 0.5]\n",
      "EPOCH= 2 BATCH= 178 G_LOSS= [13.42531967163086, 0.0] D_LOSS= [0.16264165937900543, 0.5]\n",
      "EPOCH= 2 BATCH= 179 G_LOSS= [13.344452857971191, 0.0] D_LOSS= [0.16263122856616974, 0.5]\n",
      "EPOCH= 2 BATCH= 180 G_LOSS= [13.296340942382812, 0.0] D_LOSS= [0.16262255609035492, 0.5]\n",
      "EPOCH= 2 BATCH= 181 G_LOSS= [13.404163360595703, 0.0] D_LOSS= [0.16264882683753967, 0.5]\n",
      "EPOCH= 2 BATCH= 182 G_LOSS= [13.383927345275879, 0.0] D_LOSS= [0.1626300811767578, 0.5]\n",
      "EPOCH= 2 BATCH= 183 G_LOSS= [13.265105247497559, 0.0] D_LOSS= [0.1626376062631607, 0.5]\n",
      "EPOCH= 2 BATCH= 184 G_LOSS= [13.449799537658691, 0.0] D_LOSS= [0.1627061367034912, 0.5]\n",
      "EPOCH= 2 BATCH= 185 G_LOSS= [13.426621437072754, 0.0] D_LOSS= [0.16265568137168884, 0.5]\n",
      "EPOCH= 2 BATCH= 186 G_LOSS= [13.351470947265625, 0.0] D_LOSS= [0.162643164396286, 0.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH= 2 BATCH= 187 G_LOSS= [13.485713958740234, 0.0] D_LOSS= [0.1626589447259903, 0.5]\n",
      "EPOCH= 2 BATCH= 188 G_LOSS= [13.466399192810059, 0.0] D_LOSS= [0.16264373064041138, 0.5]\n",
      "EPOCH= 2 BATCH= 189 G_LOSS= [13.37570571899414, 0.0] D_LOSS= [0.16261908411979675, 0.5]\n",
      "EPOCH= 2 BATCH= 190 G_LOSS= [13.502791404724121, 0.0] D_LOSS= [0.16265209019184113, 0.5]\n",
      "EPOCH= 2 BATCH= 191 G_LOSS= [13.466460227966309, 0.0] D_LOSS= [0.1626635491847992, 0.5]\n",
      "EPOCH= 2 BATCH= 192 G_LOSS= [13.381485939025879, 0.0] D_LOSS= [0.1626269668340683, 0.5]\n",
      "EPOCH= 2 BATCH= 193 G_LOSS= [13.557951927185059, 0.0] D_LOSS= [0.16267770528793335, 0.5]\n",
      "EPOCH= 2 BATCH= 194 G_LOSS= [13.46872329711914, 0.0] D_LOSS= [0.16268183290958405, 0.5]\n",
      "EPOCH= 2 BATCH= 195 G_LOSS= [13.382427215576172, 0.0] D_LOSS= [0.16262458264827728, 0.5]\n",
      "EPOCH= 2 BATCH= 196 G_LOSS= [13.579137802124023, 0.0] D_LOSS= [0.16269007325172424, 0.5]\n",
      "EPOCH= 2 BATCH= 197 G_LOSS= [13.43337631225586, 0.0] D_LOSS= [0.16271987557411194, 0.5]\n",
      "EPOCH= 2 BATCH= 198 G_LOSS= [13.425095558166504, 0.0] D_LOSS= [0.162626713514328, 0.5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x0000025F7E9D3160>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 545, in __del__\n",
      "    gen_dataset_ops.delete_iterator(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 1262, in delete_iterator\n",
      "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH= 2 BATCH= 199 G_LOSS= [13.573439598083496, 0.0] D_LOSS= [0.1626489907503128, 0.5]\n",
      "EPOCH= 2 BATCH= 200 G_LOSS= [13.433910369873047, 0.0] D_LOSS= [0.1627027988433838, 0.5]\n",
      "EPOCH= 2 BATCH= 201 G_LOSS= [13.460295677185059, 0.0] D_LOSS= [0.1626346856355667, 0.5]\n",
      "EPOCH= 2 BATCH= 202 G_LOSS= [13.58493423461914, 0.0] D_LOSS= [0.16263523697853088, 0.5]\n",
      "EPOCH= 2 BATCH= 203 G_LOSS= [13.407271385192871, 0.0] D_LOSS= [0.16267669200897217, 0.5]\n",
      "EPOCH= 2 BATCH= 204 G_LOSS= [13.44314193725586, 0.0] D_LOSS= [0.16264277696609497, 0.5]\n",
      "EPOCH= 2 BATCH= 205 G_LOSS= [13.503600120544434, 0.0] D_LOSS= [0.16261540353298187, 0.5]\n",
      "EPOCH= 2 BATCH= 206 G_LOSS= [13.325804710388184, 0.0] D_LOSS= [0.16267254948616028, 0.5]\n",
      "EPOCH= 2 BATCH= 207 G_LOSS= [13.42171859741211, 0.0] D_LOSS= [0.16269823908805847, 0.5]\n",
      "EPOCH= 2 BATCH= 208 G_LOSS= [13.426803588867188, 0.0] D_LOSS= [0.1626233160495758, 0.5]\n",
      "EPOCH= 2 BATCH= 209 G_LOSS= [13.215561866760254, 0.0] D_LOSS= [0.1626560389995575, 0.5]\n",
      "EPOCH= 2 BATCH= 210 G_LOSS= [13.344649314880371, 0.0] D_LOSS= [0.16271434724330902, 0.5]\n",
      "EPOCH= 2 BATCH= 211 G_LOSS= [13.324914932250977, 0.0] D_LOSS= [0.16265887022018433, 0.5]\n",
      "EPOCH= 2 BATCH= 212 G_LOSS= [13.255352020263672, 0.0] D_LOSS= [0.16261166334152222, 0.5]\n",
      "EPOCH= 2 BATCH= 213 G_LOSS= [13.308284759521484, 0.0] D_LOSS= [0.16262531280517578, 0.5]\n",
      "EPOCH= 2 BATCH= 214 G_LOSS= [13.316084861755371, 0.0] D_LOSS= [0.1626100242137909, 0.5]\n",
      "EPOCH= 2 BATCH= 215 G_LOSS= [13.211472511291504, 0.0] D_LOSS= [0.16263535618782043, 0.5]\n",
      "EPOCH= 2 BATCH= 216 G_LOSS= [13.287384986877441, 0.0] D_LOSS= [0.16263625025749207, 0.5]\n",
      "EPOCH= 2 BATCH= 217 G_LOSS= [13.374983787536621, 0.0] D_LOSS= [0.16261479258537292, 0.5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e687628c53b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mnoise2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0my2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#0대신에 1로 채워진 배열 만들어줌\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#학습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'EPOCH='\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'BATCH='\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'G_LOSS='\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'D_LOSS='\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1798\u001b[0m                                                     class_weight)\n\u001b[0;32m   1799\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1800\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1801\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1802\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#학습\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "#60000개 데이터개수를 배치사이즈로 나눔\n",
    "num_of_batches = int(X_train.shape[0] / BATCH_SIZE)\n",
    "\n",
    "for e in range(EPOCHS):#반복한것을 에폭개수만큼 봄\n",
    "    for n in range(num_of_batches): # 배치사이즈만큼 반복\n",
    "        #4.판별자 학습#\n",
    "        #가짜데이터 임의로 (재료) 생성\n",
    "        noise = np.random.normal(0,1,size=(BATCH_SIZE,100))#0~1사이 숫자를 size개 생성\n",
    "        ##배치사이즈 만큼의 실수값 (100차원)\n",
    "        #가짜데이터 생성\n",
    "        fake_data = g.predict(noise)\n",
    "        #진짜데이터 생성\n",
    "        real_data = X_train[np.random.randint(0, X_train.shape[0],size=BATCH_SIZE)]  #60000, 784-> 배치사이즈 만큼 랜덤하게 가져옴(정수값)\n",
    "        \n",
    "        #가짜,진짜데이터 합침(절반씩붙임)\n",
    "        X = np.concatenate((real_data, fake_data))\n",
    "        y = np.zeros(2 * BATCH_SIZE) #0으로 채워진 배열\n",
    "        y[:BATCH_SIZE]=0.9 #절반은 0 이아닌 1이어야함(smooding해서 딱 1이아닌 0)\n",
    "        \n",
    "        #학습이 가능한 상태로 돌려줌\n",
    "        d.trainable = True\n",
    "        d_loss = d.train_on_batch(X,y) #간에서는 fit대신 씀->학습 (손실함수값 저장되어있음)\n",
    "        \n",
    "        #5.생성자 학습# - 간 전체를 학습시키는 것\n",
    "        d.trainable = False #다시 판별자가 학습 못하게(판별만함 - 가중치 갱신X)\n",
    "        #전체의 원재료 : noise로 가짜데이터 생성해서 판별자에게 넘기는 데이터 생성\n",
    "        noise2 = np.random.normal(0,1,size=(BATCH_SIZE,100))\n",
    "        y2 = np.ones(BATCH_SIZE)#0대신에 1로 채워진 배열 만들어줌\n",
    "        g_loss = model.train_on_batch(noise2,y2) #학습\n",
    "        \n",
    "        print('EPOCH=',e+1, 'BATCH=',n+1, 'G_LOSS=',g_loss, 'D_LOSS=',d_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e5f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EPOCH= 1 BATCH= 1 G_LOSS= [6.25101900100708, 0.0] D_LOSS= [0.6248681545257568, 0.5]\n",
    "#EPOCH= 1 BATCH= 2 G_LOSS= [3.66963791847229, 0.0] D_LOSS= [0.41628673672676086, 0.5]\n",
    "#EPOCH= 1 BATCH= 3 G_LOSS= [4.668774127960205, 0.0] D_LOSS= [0.20419344305992126, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c02ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#위 방법 보다 실제로 쓰기 쉬운 코드 --> DCGAN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
